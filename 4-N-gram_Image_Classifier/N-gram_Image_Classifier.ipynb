{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش اول:*\n",
    "---\n",
    "### 1_1\n",
    "In Vision Transformer (ViT), images are tokenized by dividing them into smaller patches. Each image x with dimensions H×W×C (height, width, channels) is split into non-overlapping patches of size P×P (with stride = P). The number of patches is N = (H*W)/P^2, and each patch is flattened into a vector.\n",
    "\n",
    "These flattened patches are then projected into a latent space of D (token vector). Then a class token is added to each patch tokens, also positional embedding is used for spatial imformation of patches. Finally, the patches are processed by the Transformer, and their classes are predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\arrays\\categorical.py:528: RuntimeWarning: invalid value encountered in cast\n",
      "  fill_value = lib.item_from_zerodim(np.array(np.nan).astype(dtype))\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1_1 and 1_2\n",
    "'''\n",
    "\n",
    "\n",
    "FOR THIS QUESTION I DID NOT USE GPT\n",
    "\n",
    "\n",
    "In this question, we want to tokenize the images and using tokenized images, # 1_1\n",
    "train a image classifier. For this sake, we use the MNIST dataset, patch the \n",
    "images, cluster the patchs using kmeans and classify the tokens using a simple \n",
    "4gram.\n",
    "\n",
    "We divide our proect into 2 parts:\n",
    "\n",
    "\n",
    "Part 1: Tokenizing Images with KMeans:\n",
    "\n",
    "    1) Load the MNIST dataset.\n",
    "\n",
    "    2) Divide each image into 14x14 patchs (stride 14).\n",
    "\n",
    "    3) Use KMeans clustering to assign each patch to one of 256 clusters (tokens).\n",
    "\n",
    "    4) The result for each image will be a sequence of 16 tokens.\n",
    "\n",
    "\n",
    "Part 2: Training a 4-Gram Classifier:\n",
    "\n",
    "    1) Use the tokenized image sequences as input.\n",
    "\n",
    "    2) Train a 4-gram model on these sequences, using a classifier like Logistic \n",
    "       Regression to predict the image class.\n",
    "\n",
    "    3) Evaluate the model on the test dataset and show example predictions.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Part 1: Loading MNIST dataset and tokenizing using KMeans\n",
    "\n",
    "\n",
    "# load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X, y = mnist['data'], mnist['target']\n",
    "\n",
    "# normalize the images by deviding by 255\n",
    "X = X/255.0\n",
    "\n",
    "# reshape images to 28x28 format\n",
    "X = X.to_numpy().reshape(-1, 28, 28)\n",
    "y = y.astype(int)\n",
    "\n",
    "\n",
    "# split the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# function to extract patchs from an image\n",
    "def extract_patchs(image, patch_size=7, stride=7):\n",
    "    patchs = []\n",
    "    for i in range(0, image.shape[0], stride):\n",
    "        for j in range(0, image.shape[1], stride):\n",
    "            patch = image[i:i+patch_size, j:j+patch_size]\n",
    "\n",
    "            if patch.shape == (patch_size, patch_size):\n",
    "                patchs.append(patch.flatten())\n",
    "\n",
    "    return np.array(patchs)\n",
    "\n",
    "\n",
    "# extract patchs from training images\n",
    "patch_size, stride = 7, 7\n",
    "patchs = []\n",
    "\n",
    "for img in X_train:\n",
    "    img_patchs = extract_patchs(img, patch_size, stride)\n",
    "    patchs.append(img_patchs)\n",
    "\n",
    "patchs = np.vstack(patchs)  # flatten all patchs into a single dataset for clustering\n",
    "\n",
    "\n",
    "# apply KMeans for tokenizing the patchs\n",
    "n_clusters = 256  # set the number of clusters (tokens)\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, \n",
    "                         max_iter=1000, batch_size=128)\n",
    "kmeans.fit(patchs)\n",
    "\n",
    "\n",
    "# tokenize the training dataset\n",
    "def tokenize_image(image, patch_size=7, stride=7, \n",
    "                   clustering_model=KMeans(n_clusters=256, random_state=42)):\n",
    "    img_patchs = extract_patchs(image, patch_size, stride)\n",
    "    tokens = clustering_model.predict(img_patchs) # assign each patch to a cluster\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# tokenize the data\n",
    "tokenized_images_train = [tokenize_image(img, patch_size, stride, kmeans) \n",
    "                            for img in X_train]\n",
    "tokenized_images_test = [tokenize_image(img, patch_size, stride, kmeans) \n",
    "                            for img in X_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش دوم:*\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 4-gram classifier: 0.79\n"
     ]
    }
   ],
   "source": [
    "# # 2_1\n",
    "# # Part 2: Training a 4-Gram Model\n",
    "\n",
    "\n",
    "# # Function to generate 4-gram training data\n",
    "# def generate_4gram_data(tokenized_images, y_labels, ngram_size=4):\n",
    "#     X_ngrams = []\n",
    "#     y_ngrams = []\n",
    "    \n",
    "#     for tokens, label in zip(tokenized_images, y_labels):\n",
    "#         for i in range(len(tokens) - ngram_size + 1):\n",
    "#             ngram = tokens[i:i + ngram_size]\n",
    "#             X_ngrams.append(ngram)\n",
    "#             y_ngrams.append(label)\n",
    "    \n",
    "#     return np.array(X_ngrams), np.array(y_ngrams)\n",
    "\n",
    "# # Generate 4-gram data for training and test sets\n",
    "# X_train_ngrams, y_train_ngrams = generate_4gram_data(tokenized_images_train, \n",
    "#                                                      y_train)\n",
    "# X_test_ngrams, y_test_ngrams = generate_4gram_data(tokenized_images_test, y_test)\n",
    "\n",
    "# # Train a logistic regression classifier on the 4-gram data\n",
    "# clf = RandomForestClassifier()\n",
    "# clf.fit(X_train_ngrams, y_train_ngrams)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# y_pred = clf.predict(X_test_ngrams)\n",
    "# accuracy = accuracy_score(y_test_ngrams, y_pred)\n",
    "\n",
    "# # Show the accuracy and some sample predictions\n",
    "# print(\"Accuracy of 4-gram model:\", accuracy)\n",
    "\n",
    "# # 2_2\n",
    "# print(\"Sample predictions:\", y_pred[:10])\n",
    "# print(\"True labels:\", y_test_ngrams[:10])\n",
    "\n",
    "\n",
    "class NGramClassifier:\n",
    "    def __init__(self, n=4):\n",
    "        self.n = n\n",
    "        self.vocab = set()\n",
    "        self.ngram_counts_per_class = defaultdict(Counter)\n",
    "        self.total_ngrams_per_class = defaultdict(int)\n",
    "        self.class_counts = Counter()\n",
    "        self.classes = []\n",
    "\n",
    "    def count_ngrams(self, tokens):\n",
    "        \"\"\"Count the n-grams in the given tokens.\"\"\"\n",
    "        ngrams = Counter([tuple(tokens[i:i+self.n]) for i in range(len(tokens)-self.n+1)])\n",
    "        return ngrams\n",
    "\n",
    "    def fit(self, tokenized_images, labels):\n",
    "        \"\"\"Train the n-gram classifier by counting n-grams per class.\"\"\"\n",
    "        self.classes = list(set(labels))  # Store unique classes\n",
    "        for tokens, label in zip(tokenized_images, labels):\n",
    "            ngrams = self.count_ngrams(tokens)\n",
    "            self.ngram_counts_per_class[label].update(ngrams)\n",
    "            self.total_ngrams_per_class[label] += sum(ngrams.values())\n",
    "            self.vocab.update(ngrams)\n",
    "            self.class_counts[label] += 1\n",
    "\n",
    "    def calculate_class_prob(self, ngrams, class_label):\n",
    "        \"\"\"Calculate the log probability of the class given the n-grams.\"\"\"\n",
    "        total_ngrams = self.total_ngrams_per_class[class_label]\n",
    "        class_prob = np.log(self.class_counts[class_label] / sum(self.class_counts.values()))  # P(class)\n",
    "\n",
    "        log_prob = class_prob  # Start with prior\n",
    "        for ngram, count in ngrams.items():\n",
    "            # Use Laplace smoothing for unseen n-grams\n",
    "            ngram_count = self.ngram_counts_per_class[class_label].get(ngram, 0) + 1\n",
    "            prob = ngram_count / (total_ngrams + len(self.vocab))  # P(ngram | class)\n",
    "            log_prob += count * np.log(prob)  # log(P(ngram | class)) * count\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "    def predict(self, tokenized_images):\n",
    "        \"\"\"Predict the class for each input image based on n-grams.\"\"\"\n",
    "        predictions = []\n",
    "        for tokens in tokenized_images:\n",
    "            ngrams = self.count_ngrams(tokens)\n",
    "            class_probs = {cls: self.calculate_class_prob(ngrams, cls) for cls in self.classes}\n",
    "            predicted_class = max(class_probs, key=class_probs.get)\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def evaluate(self, tokenized_images, labels):\n",
    "        \"\"\"Evaluate the classifier on a test set.\"\"\"\n",
    "        predictions = self.predict(tokenized_images)\n",
    "        accuracy = np.mean(np.array(predictions) == np.array(labels))\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "# Train and evaluate the NGram classifier\n",
    "ngram_classifier = NGramClassifier(n=4)\n",
    "ngram_classifier.fit(tokenized_images_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "accuracy = ngram_classifier.evaluate(tokenized_images_test, y_test)\n",
    "print(f\"Accuracy of 4-gram classifier: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
