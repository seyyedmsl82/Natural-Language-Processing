{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش دوم:*\n",
    "---\n",
    "### 1_1\n",
    "WordPiece is a subword tokenization algorithm mostly used for large vocabularies with complex words. It is closely related to BPE and was introduced by Google. It starts with a vocabulary of characters from the training corpus, adds special tokens (such as [UNK] for unknown words and ## prefix for subwords that do not start a word), splits the corpus into words, and creates all possible subword combinations for each word. Then, it calculates token frequencies and performs a merging process, which is the core of the algorithm. In the merging process, a score is assigned to each pair, and pairs with the highest score are selected, merged, and added to the vocabulary. This process continues until it reaches a predefined vocabulary size or until the highest score falls below a certain threshold. Once the final vocabulary is constructed, the tokenization algorithm begins.\n",
    "\n",
    "### 1_2\n",
    "Some popular models like BERT, DistilBERT (a lighter and faster version of BERT), ALBERT (A Lite BERT), and Google's Multilingual BERT have utilized WordPiece encoding as a crucial component.\n",
    "\n",
    "### 1_3\n",
    "WordPiece and BPE are very similar to each other but have some differences:\n",
    "\n",
    "        1-Token selection method:\n",
    "        WordPiece selects tokens based on their probability of occurrence and their combinations using statistical probabilities, whereas BPE operates by merging the most \n",
    "        frequent pairs of tokens at each step.\n",
    "\n",
    "        2-Efficiency and speed:\n",
    "        Being more complex and involving statistical calculations, WordPiece may be slower but produces more accurate results. In contrast, BPE is faster since its method is \n",
    "        based on frequency.\n",
    "\n",
    "        3-Quality and results:\n",
    "        WordPiece demonstrates better quality on complex languages or large datasets, whereas BPE performs poorly when it encounters complex datasets.\n",
    "\n",
    "        4-Usage in language models:\n",
    "        BPE is mostly used in GPT variants, but WordPiece is typically used in BERT and other transformer-based architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99218/99218 [00:00<00:00, 998863.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./tokenizer\\\\vocab.txt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2_1:      ## Has been inspired by a youtube video which learnt how to tokenize using wordpeice\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# os.mkdir('./Ferdowsi')\n",
    "with open(f'./data/ferdowsi.txt', 'r', encoding='utf-8') as fp:\n",
    "    data = fp.readlines()\n",
    "\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "for sample in tqdm(data):\n",
    "    sample = sample.split('\\n')\n",
    "    sample = sample[0].split('|')\n",
    "    text_data.append(sample[1])\n",
    "    if len(text_data) == 5000:\n",
    "        with open(f'./Ferdowsi/file_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        \n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "\n",
    "with open(f'./Ferdowsi/file_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))\n",
    "\n",
    "\n",
    "paths = [os.path.join('./Ferdowsi', file) for file in os.listdir('./Ferdowsi')]\n",
    "\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    strip_accents=False\n",
    ")\n",
    "\n",
    "tokenizer.train(\n",
    "    files=paths,\n",
    "    vocab_size=20000,\n",
    "    min_frequency=10,\n",
    "    special_tokens=[\n",
    "        '[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'\n",
    "    ],\n",
    "    wordpieces_prefix='##'\n",
    "    )\n",
    "\n",
    "# os.mkdir('./tokenizer')\n",
    "tokenizer.save_model('./tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      " در\n",
      " نخستین\n",
      " سا\n",
      " ##عت\n",
      " شب\n",
      " ،\n",
      " در\n",
      " ا\n",
      " ##ط\n",
      " ##اق\n",
      " چوب\n",
      " ##یش\n",
      " تنها\n",
      " ،\n",
      " زن\n",
      " چینی\n",
      " در\n",
      " سرش\n",
      " اندیشه\n",
      " های\n",
      " هول\n",
      " ##ن\n",
      " ##اک\n",
      " ##ی\n",
      " دور\n",
      " می\n",
      " گیرد\n",
      " [SEP]\n",
      " "
     ]
    }
   ],
   "source": [
    "# 2_2:\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./tokenizer')\n",
    "with open('./tokenizer/vocab.txt', 'r', encoding='utf-8') as fp:\n",
    "    vocab = fp.readlines()\n",
    "\n",
    "text = 'در نخستین ساعت شب،  در اطاق چوبیش تنها، زن چینی در سرش اندیشه های هولناکی دور می گیرد'\n",
    "for i in tokenizer(text)['input_ids']:\n",
    "    print(vocab[i], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش سوم:*\n",
    "---\n",
    "### 3_3\n",
    "Increasing n in an n-gram model leads to several important problems, including:\n",
    "\n",
    "        1-Data sparsity (because it requires more data to cover all possible combinations), leading to zero counts and perplexity calculation issues.\n",
    "        \n",
    "        2-Increased computational cost.\n",
    "        \n",
    "        3-Overfitting (failing to generalize well to new data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3_1:\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "\n",
    "class NGram:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.tokenize(text)\n",
    "\n",
    "    def calculate_ngram_probabilities(self, train_tokens, test_tokens, n, k=1):  ## Since I did not know how consider the probability matrix \n",
    "                                                                                 ## (to consider all possible combinations), I took a hand from \n",
    "                                                                                 ## GPT, But this is my code\n",
    "        V = len(set(train_tokens))\n",
    "        \n",
    "        ngram_counts = Counter([tuple(train_tokens[i:i+n]) for i in range(len(train_tokens)-n+1)])\n",
    "        n_minus_one_gram_counts = Counter([tuple(train_tokens[i:i+n-1]) for i in range(len(train_tokens)-n)])\n",
    "        ngram_probabilities = {}\n",
    "\n",
    "        for ngram in ngram_counts:\n",
    "            prefix = ngram[:-1]\n",
    "            # ngram_counts[ngram] += k\n",
    "            # n_minus_one_gram_counts[prefix] += k\n",
    "            ngram_probabilities[ngram] = (ngram_counts[ngram] + k) / (n_minus_one_gram_counts[prefix] + k*V)\n",
    "\n",
    "\n",
    "        for i in range(len(test_tokens)-n+1):\n",
    "            ngram = tuple(test_tokens[i:i+n])\n",
    "            if ngram not in ngram_counts:\n",
    "                ngram_counts[ngram] = 0\n",
    "                prefix = ngram[:-1]\n",
    "                if prefix not in n_minus_one_gram_counts:\n",
    "                    n_minus_one_gram_counts[prefix] = 0\n",
    "\n",
    "                ngram_probabilities[ngram] = (ngram_counts[ngram] + k) / (n_minus_one_gram_counts[prefix] + k*V)\n",
    "        \n",
    "        return ngram_probabilities\n",
    "\n",
    "\n",
    "\n",
    "    def generate_text(self, ngram_probabilities, n, num_words=200): \n",
    "        ## This part has mostly generated by ChatGPT. But it was my idea. ##\n",
    "        \n",
    "        \n",
    "        # Start the text generation with a random n-gram as a seed\n",
    "        current_ngram = random.choice(list(ngram_probabilities.keys()))\n",
    "        generated_tokens = list(current_ngram)\n",
    "\n",
    "        for _ in range(num_words - n):  # Generate the remaining tokens\n",
    "            prefix = tuple(generated_tokens[-(n-1):])  # Get the last (n-1) tokens as the prefix\n",
    "            possible_ngrams = {ngram: prob for ngram, prob in ngram_probabilities.items() \n",
    "                                if ngram[:-1] == prefix}\n",
    "\n",
    "            if not possible_ngrams:\n",
    "                break  # If no possible next tokens, stop generation\n",
    "            \n",
    "            # Choose the next token based on the probabilities\n",
    "            next_ngram = random.choices(list(possible_ngrams.keys()), \n",
    "                                        weights=list(possible_ngrams.values()))[0]\n",
    "            generated_tokens.append(next_ngram[-1])  # Append the last token of the selected n-gram\n",
    "\n",
    "\n",
    "        clean_generated_tokens = []  ## From here is mine ##\n",
    "        for token in generated_tokens:\n",
    "            if token.startswith('##'):\n",
    "                clean_generated_tokens[-1] = clean_generated_tokens[-1]+token[2:]\n",
    "\n",
    "            else:\n",
    "                clean_generated_tokens.append(token)\n",
    "\n",
    "        return ' '.join(clean_generated_tokens)\n",
    "\n",
    "\n",
    "ngram_model = NGram(tokenizer)\n",
    "\n",
    "\n",
    "with open('data/ferdowsi.txt', 'r', encoding='utf-8') as fp:\n",
    "    train_text = fp.read()\n",
    "\n",
    "\n",
    "with open('data/hafez.txt', 'r', encoding='utf-8') as fp:\n",
    "    test_text = fp.read()\n",
    "\n",
    "train_tokens = ngram_model.tokenize(train_text)\n",
    "test_tokens = ngram_model.tokenize(test_text)\n",
    "\n",
    "bigram_probabilities = ngram_model.calculate_ngram_probabilities(train_tokens, test_tokens, 2)\n",
    "fourgram_probabilities = ngram_model.calculate_ngram_probabilities(train_tokens, test_tokens, 4)\n",
    "eightgram_probabilities = ngram_model.calculate_ngram_probabilities(train_tokens, test_tokens, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چون رفتی امروز و چون امدی [UNK] بر ان برترین نام یزدان پاک [UNK] سر دشمنان را بگاز اورم [UNK] بشد با گرازه به اورد رفتند پیچان عنان [UNK] همان نیزه و خود و گوپال و زین [UNK] بزر افسر و خسروانی نگین [UNK] چو دانا توانا بد و دادگر [UNK] چنان کن که نیکاختر و رای تست [UNK] زمانه بزیر کف پای تست [UNK] یکی نامه بنوشت با درد دل سام پیر [UNK] اگر هست بیهوده منمای دست [UNK] سرت پر ز تیزی و ارام من [UNK] ز پرده بگسترد بر انجمن [UNK] جهاندار بر شاد و رد بزرگ [UNK] نوشته همه نام تو بر نگین [UNK] هران بند کز دست تو کس نرست [UNK] به هر سو دورانی میکرد عشق گرم خون بخورند ناکسم گر به [UNK] سوی [UNK] روم بعد از روزگار ما بسی گردش کند گردون بسی [UNK] و نهار ارد عماری دار [UNK] را که صدر مجلس عشرت اشارتی چشمی بدان دو گوشه ابرو [UNK] ما مصلحت و فوق وجود خودم از [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "# 3_2:\n",
    "# Generate a 200-word text\n",
    "generated_text = ngram_model.generate_text(fourgram_probabilities, 4, num_words=200)\n",
    "with open('sample_generated.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write(generated_text)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش چهارم:*\n",
    "---\n",
    "### 4_1\n",
    "Perplexity is an intrinsic measure to evaluate the performance of a language model. By calculating the inverse of the geometric mean of a token's probability, it represents the number of options for the next word in a sentence, known as the branching factor. A low perplexity indicates that the model has fewer options for the next word, meaning it assigns higher probabilities to certain tokens and aims to predict a meaningful sentence.\n",
    "\n",
    "\n",
    "### 4_2\n",
    "As demonstrated by the model, increasing n in an n-gram model leads to worse perplexity due to data sparsity and other drawbacks. Additionally, ngrams are not supposed to predict the test texts out of the training dataset. This leads to low accuracy for prediction, as it has been shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hafez:\n",
      "\t2gram:\n",
      "\t\tAverage Perplexity: 2807.9586820143427\n",
      "Hafez:\n",
      "\t4gram:\n",
      "\t\tAverage Perplexity: 5227.7204785414915\n",
      "Hafez:\n",
      "\t8gram:\n",
      "\t\tAverage Perplexity: 5249.969919794957\n",
      "Modern Poet:\n",
      "\t2gram:\n",
      "\t\tAverage Perplexity: 3164.262025015781\n",
      "Modern Poet:\n",
      "\t4gram:\n",
      "\t\tAverage Perplexity: 5239.414227557773\n",
      "Modern Poet:\n",
      "\t8gram:\n",
      "\t\tAverage Perplexity: 5249.998681621938\n"
     ]
    }
   ],
   "source": [
    "# 4_2:\n",
    "def calculate_perplexity(test_tokens, ngram_probabilities, n):  \n",
    "    log_probability_sum = 0\n",
    "    ngram_count = 0\n",
    "    \n",
    "    for i in range(len(test_tokens)-n+1):\n",
    "        ngram = tuple(test_tokens[i:i+n])\n",
    "        log_probability_sum += math.log2(ngram_probabilities[ngram])\n",
    "        ngram_count += 1\n",
    "    \n",
    "    average_log_probability = -log_probability_sum / ngram_count\n",
    "    perplexity = math.pow(2, average_log_probability)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "\n",
    "\n",
    "for dataset_address, dataset_name in zip(['data/hafez.txt', 'data/modern_poet.txt'], \n",
    "                                         ['Hafez', 'Modern Poet']):\n",
    "    with open(dataset_address, 'r', encoding='utf-8') as fp:\n",
    "        test_text = fp.read()\n",
    "\n",
    "    with open(dataset_address, 'r', encoding='utf-8') as fp:\n",
    "        lines = fp.readlines()\n",
    "\n",
    "    for n in [2, 4, 8]:\n",
    "        ngram_model = NGram(tokenizer)\n",
    "\n",
    "\n",
    "        with open('data/ferdowsi.txt', 'r', encoding='utf-8') as fp:\n",
    "            train_text = fp.read()\n",
    "\n",
    "        train_tokens = ngram_model.tokenize(train_text)\n",
    "        test_tokens = ngram_model.tokenize(test_text)\n",
    "\n",
    "        total_perplexity = 0\n",
    "        line_count = 0\n",
    "        ngram_probabilities = ngram_model.calculate_ngram_probabilities(train_tokens, test_tokens, n)\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            test_tokens = ngram_model.tokenize(line.strip())\n",
    "\n",
    "            if len(test_tokens) >= n:\n",
    "                perplexity = calculate_perplexity(test_tokens, ngram_probabilities, n)\n",
    "                total_perplexity += perplexity\n",
    "                line_count += 1\n",
    "\n",
    "            \n",
    "        if line_count > 0:\n",
    "            average_perplexity = total_perplexity / line_count\n",
    "            print(f'{dataset_name}:')\n",
    "            print(f'\\t{n}gram:')\n",
    "            print(f'\\t\\tAverage Perplexity: {average_perplexity}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
