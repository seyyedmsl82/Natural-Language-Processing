{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش اول:*\n",
    "---\n",
    "### 1_1\n",
    "Using n-grams as features for classification is common in NLP. For this purpose, after tokenization, all possible n-grams from the text are calculated, and using their frequencies, each of them is mapped to feature vectors. After these steps, the feature vectors are ready for classification using their labels.\n",
    "\n",
    "### 1_2\n",
    "\n",
    "When using higher n-grams (like trigrams) with a small dataset, several problems can occur:\n",
    "\n",
    "        1-Data sparsity: Many n-grams might appear rarely, leading to weaker models.\n",
    "        \n",
    "        2-Overfitting: The model may learn patterns specific to the training data and fail to generalize.\n",
    "        \n",
    "        3-Slower training: The feature space becomes too large, making training slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش دوم:*\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Score  Suggestion\n",
      "count  3261.000000  3261.00000\n",
      "mean     74.719411     1.41061\n",
      "std      21.514015     0.72408\n",
      "min       0.000000     1.00000\n",
      "25%      60.000000     1.00000\n",
      "50%      80.000000     1.00000\n",
      "75%      92.000000     2.00000\n",
      "max     100.000000     3.00000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "data = pd.read_csv('data/digikala.csv')\n",
    "\n",
    "print(data.describe())\n",
    "\n",
    "texts = data['Text'].tolist()\n",
    "labels = data['Suggestion'].tolist()\n",
    "\n",
    "# 3_2\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, \n",
    "                                                                      test_size=0.2, random_state=42)\n",
    "\n",
    "# 2_1\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train_from_iterator(train_texts, vocab_size=20000, min_frequency=10)\n",
    "\n",
    "# 2_2\n",
    "train_tokens = [tokenizer.encode(text).tokens for text in train_texts]\n",
    "test_tokens = [tokenizer.encode(text).tokens for text in test_texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش سوم:*\n",
    "---\n",
    "### 3_4\n",
    "Due to the imbalancity of dataset, the classifier prioritize the majority class and ignore the minorities. Therefore it leads to high precidion and low recall. \n",
    "Precision being high means when the model makes a positive prediction, it is usually correct.\n",
    "Recall being low means our model has missed many true positives.\n",
    "\n",
    "### 3_5\n",
    "In these situations, it is better to use ngram classifiers instead of complex classifiers with many parameters:\n",
    "\n",
    "        1- When the data is limited\n",
    "\n",
    "        2- When calculation and speed are more important\n",
    "\n",
    "        3- When the patterns are simple and strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "2-Gram Model - Accuracy: 0.75, Precision: 0.67, Recall: 0.42\n"
     ]
    }
   ],
   "source": [
    "# 3_1\n",
    "\n",
    "class NGramClassifier:\n",
    "    def __init__(self, n=2, classifier=svm.SVC()):\n",
    "        self.n = n\n",
    "        self.vocab = set()\n",
    "        self.classifier = classifier\n",
    "        self.ngram_probabilities = {}\n",
    "\n",
    "\n",
    "    def count_ngrams(self, tokens, n):\n",
    "        ngrams = Counter([tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)])\n",
    "        return ngrams\n",
    "\n",
    "    \n",
    "    def generate_feature_vector(self, ngrams):  ## I used GPT to generate feature vector. ##\n",
    "        self.vocab = list(self.vocab)\n",
    "        feature_vector = np.zeros(len(self.vocab), dtype=int)\n",
    "        \n",
    "        # Populate the feature vector with the count of each n-gram from the vocab\n",
    "        for i, ngram in enumerate(self.vocab):\n",
    "            # If the n-gram exists in the current ngram dictionary, get its count, else use 0\n",
    "            feature_vector[i] = ngrams.get(ngram, 0)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "        \n",
    "    def fit(self, text_tokens, labels):\n",
    "        all_ngrams = []\n",
    "        for tokens in text_tokens:\n",
    "            ngrams = self.count_ngrams(tokens, self.n)\n",
    "            all_ngrams.append(ngrams)\n",
    "            self.vocab.update(ngrams)\n",
    "\n",
    "        X = np.array([self.generate_feature_vector(ngrams) for ngrams in all_ngrams])\n",
    "        # print(X.shape)\n",
    "\n",
    "        print(max(max(X[i]) for i in range(2608)))\n",
    "\n",
    "        y = np.array(labels)\n",
    "\n",
    "        self.classifier.fit(X, y)\n",
    "\n",
    "\n",
    "    def predict(self, texts):\n",
    "        all_ngrams = [self.count_ngrams(tokens, self.n) for tokens in texts]\n",
    "        X = np.array([self.generate_feature_vector(ngrams) for ngrams in all_ngrams])\n",
    "        predictions = self.classifier.predict(X)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    # 3_3\n",
    "    def evaluate(self, texts, labels):\n",
    "        predictions = self.predict(texts)\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions, average='macro')\n",
    "        recall = recall_score(labels, predictions, average='macro')\n",
    "\n",
    "        return accuracy, precision, recall\n",
    "\n",
    "\n",
    "# 3_2\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "bigram_model = NGramClassifier(n=2, classifier=classifier)\n",
    "bigram_model.fit(train_tokens, train_labels)\n",
    "\n",
    "accuracy_2gram, precision_2gram, recall_2gram = bigram_model.evaluate(test_tokens, test_labels) # 3_3\n",
    "print(f\"2-Gram Model - Accuracy: {accuracy_2gram:.2f}, Precision: {precision_2gram:.2f}, Recall: {recall_2gram:.2f}\")\n",
    "\n",
    "\n",
    "threegram_model = NGramClassifier(n=3, classifier=classifier)\n",
    "threegram_model.fit(train_tokens, train_labels)\n",
    "\n",
    "accuracy_3gram, precision_3gram, recall_3gram = threegram_model.evaluate(test_tokens, test_labels) # 3_3\n",
    "print(f\"3-Gram Model - Accuracy: {accuracy_3gram:.2f}, Precision: {precision_3gram:.2f}, Recall: {recall_3gram:.2f}\")\n",
    "\n",
    "# from collections import defaultdict, Counter\n",
    "# import numpy as np\n",
    "\n",
    "# class NGramClassifier:\n",
    "#     def __init__(self, n=2):\n",
    "#         self.n = n\n",
    "#         self.vocab = set()\n",
    "#         self.ngram_counts_per_class = defaultdict(Counter)\n",
    "#         self.total_ngrams_per_class = defaultdict(int)\n",
    "#         self.class_counts = Counter()\n",
    "#         self.classes = []\n",
    "\n",
    "#     def count_ngrams(self, tokens, n):\n",
    "#         \"\"\"Count the n-grams in the given tokens.\"\"\"\n",
    "#         ngrams = Counter([tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)])\n",
    "#         return ngrams\n",
    "\n",
    "#     def fit(self, text_tokens, labels):\n",
    "#         \"\"\"Train the n-gram classifier by counting n-grams per class.\"\"\"\n",
    "#         self.classes = list(set(labels))  # Store unique classes\n",
    "#         for tokens, label in zip(text_tokens, labels):\n",
    "#             ngrams = self.count_ngrams(tokens, self.n)\n",
    "#             self.ngram_counts_per_class[label].update(ngrams)\n",
    "#             self.total_ngrams_per_class[label] += sum(ngrams.values())\n",
    "#             self.vocab.update(ngrams)\n",
    "#             self.class_counts[label] += 1\n",
    "\n",
    "#     def calculate_class_prob(self, ngrams, class_label):\n",
    "#         \"\"\"Calculate the log probability of the class given the n-grams.\"\"\"\n",
    "#         total_ngrams = self.total_ngrams_per_class[class_label]\n",
    "#         class_prob = np.log(self.class_counts[class_label] / sum(self.class_counts.values()))  # P(class)\n",
    "\n",
    "#         log_prob = class_prob  # Start with prior\n",
    "#         for ngram, count in ngrams.items():\n",
    "#             # Use Laplace smoothing for unseen n-grams\n",
    "#             ngram_count = self.ngram_counts_per_class[class_label].get(ngram, 0) + 1\n",
    "#             prob = ngram_count / (total_ngrams + len(self.vocab))  # P(ngram | class)\n",
    "#             log_prob += count * np.log(prob)  # log(P(ngram | class)) * count\n",
    "\n",
    "#         return log_prob\n",
    "\n",
    "#     def predict(self, texts):\n",
    "#         \"\"\"Predict the class for each input text based on n-grams.\"\"\"\n",
    "#         predictions = []\n",
    "#         for tokens in texts:\n",
    "#             ngrams = self.count_ngrams(tokens, self.n)\n",
    "#             class_probs = {cls: self.calculate_class_prob(ngrams, cls) for cls in self.classes}\n",
    "#             predicted_class = max(class_probs, key=class_probs.get)\n",
    "#             predictions.append(predicted_class)\n",
    "\n",
    "#         return predictions\n",
    "\n",
    "#     def evaluate(self, texts, labels):\n",
    "#         \"\"\"Evaluate the classifier on a test set.\"\"\"\n",
    "#         predictions = self.predict(texts)\n",
    "#         accuracy = np.mean(np.array(predictions) == np.array(labels))\n",
    "#         return accuracy\n",
    "\n",
    "# # Example usage\n",
    "# ngram_model = NGramClassifier(n=2)\n",
    "# ngram_model.fit(train_tokens, train_labels)\n",
    "\n",
    "# # Evaluate model\n",
    "# accuracy = ngram_model.evaluate(test_tokens, test_labels)\n",
    "# print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
