{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش اول:*\n",
    "---\n",
    "### 1_1\n",
    "NER is a technique in NLP that locates and classifies named entities from unstructured data into different predefined categories, namely persons, organizations, times, locations, etc. First, NER analyzes the input text to identify and locate named entities, then, using capitalization rules, it identifies sentence boundaries. In the third step, it classifies documents into different types. Finally, using ML algorithms, it recognizes entities in new data and improves its accuracy by passing through multiple training iterations. Some NER methods are lexicon-based, rule-based, ML-based, and DL-based. NER is available in Python using the spaCy library.\n",
    "\n",
    "### 1-3\n",
    "Utilizing Regex in NER tasks has several benefits and drawbacks.\n",
    "\n",
    "Since it provides direct control over patterns in text, training is not required and, therefore, it offers fast processing and high efficiency for simple patterns. However, because of its poor contextual understanding and limited flexibility, it may miss important entities; hence, it is not ideal for difficult tasks or large-scale datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['info@vrinnovations.c', 'martinez@techadvisors.n', 'support@innovateads.c', 'p@mailmasters.c', 'sales@mailmasters.c', 'director@socialbuzz.o', 'support@deepad.a', 'info@legaleagle.o', 'partners@influenceme.i', 'contact@brandvision.c', 'hello@mobilemarketersinc.c', 'jenkins@cxinnovators.c', 'info@digitalfuture2025.c'] ['+1-800', '+44 20', '+1-310', '+1-212', '+1-718', '+49 30', '+33 1', '+61 3', '+1-202', '+1-415', '+1-646', '+1-888'] ['https://www.digitalfuture2025.com', 'http://www.techadvisors.net', 'https://www.innovateads.co', 'https://www.innovateads.co', 'http://www.mailmasters.com', 'https://www.socialbuzz.org', 'http://www.deepad.ai', 'http://www.deepad.ai', 'https://www.legaleagle.org', 'http://www.influenceme.io', 'https://www.brandvision.com', 'http://www.mobilemarketersinc.com', 'https://www.cxinnovators.com', 'https://www.cxinnovators.com', 'https://www.digitalfuture2025.com']\n"
     ]
    }
   ],
   "source": [
    "# 1_2:\n",
    "import re\n",
    "\n",
    "file_path = r'data\\ner.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "email_pattern = r'[a-zA-Z0-9]+@[a-zA-Z0-9]+\\.[a-zA-Z]'\n",
    "phone_pattern = r'\\+[0-9]+[\\s -0-9]*[0-9]+'\n",
    "url_pattern = r'https?://www\\.[a-zA-Z0-9.]*[a-zA-Z0-9]+\\.[a-zA-Z]+'\n",
    "\n",
    "emails = re.findall(email_pattern, text)\n",
    "phones = re.findall(phone_pattern, text)\n",
    "urls = re.findall(url_pattern, text)\n",
    "\n",
    "print(emails, phones, urls)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش دوم:*\n",
    "---\n",
    "### 2_1\n",
    "Rule-based tokenizers use a set of predefined linguistic rules and grammars (mostly whitespace, punctuation, and capitalization) to split the input text into tokens. Their results are remarkable on well-structured datasets with predictable patterns. Since training is not required, these tokenizers are very fast and efficient. However, the necessity of rules makes them inflexible.\n",
    "\n",
    "Machine learning-based tokenizers use ML algorithms to learn patterns from training data (mostly considering morphology and context). Hence, they are typically accurate on complex datasets and adaptable to real-world tasks due to their ability to handle informality. They are also highly flexible and adaptable to new domains. However, they may require resources, data preparation, and training processes, and they significantly impact tokenizing speed.\n",
    "\n",
    "Rule-based tokenizers are best for applications where the text follows strict patterns (like formal writing).\n",
    "\n",
    "ML-based tokenizers are ideal for NLP tasks where context is important.\n",
    "\n",
    "\n",
    "### 2_2\n",
    "Whitespace tokenizer is not suitable in complex languages which does not have space for each word (ie. Chinese and Japanese). Also it is not proper for combination of subwords and can not make new combination for an unseen word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 2_2_ Whitespace Tokenization Question\n",
    "\n",
    "whitespace_tokens = text.split() # Built-in function\n",
    "\n",
    "\n",
    "def whitespace_tokenizer(text): # custom function\n",
    "    tokens = []\n",
    "    token = \"\"\n",
    "    \n",
    "    for char in text:\n",
    "        if not char.isspace():\n",
    "            token += char\n",
    "        else:\n",
    "            if token:\n",
    "                tokens.append(token)\n",
    "                token = \"\"\n",
    "    \n",
    "    if token:\n",
    "        tokens.append(token)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "tokens = whitespace_tokenizer(text)\n",
    "\n",
    "print(tokens == whitespace_tokens) # compare built-in and custom functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش سوم:*\n",
    "---\n",
    "### 3_1\n",
    "Levenshtein distance refers to the minimum number of operations required to transform one string into another. The allowed operations are insertion, deletion, and substitution.\n",
    "\n",
    "Damerau-Levenshtein distance is an extension of Levenshtein distance that adds one extra operation (transposition) for swapping two adjacent characters. This distance is suitable for correcting human typing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for kitten and sitting:\n",
      "\tlevenshtein_distance is 5\n",
      "\tdamerau_levenshtein_distance is 5\n",
      "for saturday and sunday:\n",
      "\tlevenshtein_distance is 4\n",
      "\tdamerau_levenshtein_distance is 4\n",
      "for book and back:\n",
      "\tlevenshtein_distance is 4\n",
      "\tdamerau_levenshtein_distance is 4\n",
      "for algorithm and logarithm:\n",
      "\tlevenshtein_distance is 4\n",
      "\tdamerau_levenshtein_distance is 3\n",
      "for  and test:\n",
      "\tlevenshtein_distance is 4\n",
      "\tdamerau_levenshtein_distance is 4\n",
      "for abc and acb:\n",
      "\tlevenshtein_distance is 2\n",
      "\tdamerau_levenshtein_distance is 1\n"
     ]
    }
   ],
   "source": [
    "# 3_2:\n",
    "def levenshtein_distance(str1, str2):\n",
    "    distance_matrix = [[0 for _ in range(len(str2)+1)] for _ in range(len(str1)+1)]\n",
    "\n",
    "    # initialization\n",
    "    for i in range(len(str1)+1):\n",
    "        distance_matrix[i][0] = i\n",
    "    for j in range(len(str2)+1):\n",
    "        distance_matrix[0][j] = j\n",
    "\n",
    "    for i in range(1, len(str1)+1):\n",
    "        for j in range(1, len(str2)+1):\n",
    "            if str1[i-1] == str2[j-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 2\n",
    "\n",
    "            distance_matrix[i][j] = min(\n",
    "                distance_matrix[i-1][j] + 1,    # Deletion\n",
    "                distance_matrix[i][j-1] + 1,    # Insertion\n",
    "                distance_matrix[i-1][j-1] + cost  # Substitution\n",
    "            )\n",
    "    return distance_matrix[len(str1)][len(str2)]\n",
    "\n",
    "\n",
    "def damerau_levenshtein_distance(str1, str2):\n",
    "    distance_matrix = [[0 for _ in range(len(str2) + 1)] for _ in range(len(str1) + 1)]\n",
    "    \n",
    "    for i in range(len(str1) + 1):\n",
    "        distance_matrix[i][0] = i\n",
    "    for j in range(len(str2) + 1):\n",
    "        distance_matrix[0][j] = j\n",
    "\n",
    "    for i in range(1, len(str1) + 1):\n",
    "        for j in range(1, len(str2) + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 2\n",
    "\n",
    "            distance_matrix[i][j] = min(\n",
    "                distance_matrix[i - 1][j] + 1,      # Deletion\n",
    "                distance_matrix[i][j - 1] + 1,      # Insertion\n",
    "                distance_matrix[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "\n",
    "            if str1[i - 1] == str2[j - 2] and str1[i - 2] == str2[j - 1] and i > 1 and j > 1:\n",
    "                distance_matrix[i][j] = min(\n",
    "                    distance_matrix[i][j],\n",
    "                    distance_matrix[i - 2][j - 2] + 1  # Transposition\n",
    "                )\n",
    "\n",
    "    return distance_matrix[len(str1)][len(str2)]\n",
    "\n",
    "\n",
    "texts = [('kitten', 'sitting'), ('saturday', 'sunday'), ('book', 'back'), ('algorithm', 'logarithm'),\n",
    "         ('', 'test'), ('abc', 'acb')]\n",
    "\n",
    "for str1, str2 in texts:\n",
    "    levenshtein = levenshtein_distance(str1, str2)\n",
    "    damerau_levenshtein = damerau_levenshtein_distance(str1, str2)\n",
    "\n",
    "    print(f'for {str1} and {str2}:')\n",
    "    print(f'\\tlevenshtein_distance is {levenshtein}')\n",
    "    print(f'\\tdamerau_levenshtein_distance is {damerau_levenshtein}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
