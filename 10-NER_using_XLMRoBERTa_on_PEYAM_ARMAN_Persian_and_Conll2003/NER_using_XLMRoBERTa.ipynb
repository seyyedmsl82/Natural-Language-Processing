{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **مجموعه داده:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px\">\n",
    "\n",
    "*بخش اول:*\n",
    "---\n",
    "مشکلات کلی که مشاهده می‌شود:\n",
    "\n",
    "1. تفاوت در ساختار و ویژگی‌ها:\n",
    "    * دیتاست فارسی شامل ستون‌های tokens، ner_tags، و ner_tags_names است، در حالی که دیتاست انگلیسی شامل ستون‌های id، tokens، pos_tags، chunk_tags و ner_tags است.\n",
    "    * این تفاوت باعث می‌شود که در هنگام ادغام، نیاز به تنظیم ساختار داده‌ها باشد.\n",
    "\n",
    "2. تفاوت در برچسب‌های ner_tags:\n",
    "    * برچسب‌های ner_tags در دیتاست فارسی شامل انواعی مانند B_LOC، B_PRO و غیره است.\n",
    "    * برچسب‌های دیتاست انگلیسی با نام‌های مختلف مانند B-PER، B-LOC و غیره تعریف شده‌اند. این تفاوت در نام‌گذاری می‌تواند مشکلاتی در ادغام داده‌ها ایجاد کند.\n",
    "\n",
    "3. وجود برچسب‌های اضافی در دیتاست فارسی:\n",
    "    * دیتاست فارسی برچسب‌هایی مانند B_EVE، I_EVE، B_PRO، I_PRO و غیره دارد که در دیتاست انگلیسی وجود ندارند.\n",
    "    * این برچسب‌ها باید به یک مجموعه عمومی از برچسب‌ها نگاشت شوند تا مدل بتواند از آن‌ها استفاده کند.\n",
    "\n",
    "4. عدم هم‌ترازی داده‌ها:\n",
    "    * در دیتاست فارسی، ستون ner_tags_names وجود دارد، اما در دیتاست انگلیسی فقط ner_tags و کلاس‌های آن وجود دارند.\n",
    "\n",
    "---\n",
    "\n",
    "راه‌حل‌های پیشنهادی:\n",
    "\n",
    "1. **استانداردسازی برچسب‌ها**: استفاده از یک نقشه کلی (general_mapping) که تمامی برچسب‌های هر دو دیتاست را به یک فضای مشترک نگاشت کند. این نقشه باید شامل تمامی برچسب‌های ممکن باشد.\n",
    "\n",
    "\n",
    "2. **هماهنگ‌سازی ساختار داده‌ها**: ایجاد ستون‌های مورد نیاز (مانند ner_tags_names در دیتاست انگلیسی) یا حذف ستون‌های اضافی برای همگام‌سازی ساختار داده‌ها.\n",
    "\n",
    "3. **حذف یا نگاشت برچسب‌های غیرمشترک**:  برچسب‌هایی که فقط در دیتاست فارسی وجود دارند، مانند B_EVE و I_PRO، باید به یک برچسب عمومی (مانند O) نگاشت شوند یا حذف شوند.\n",
    "\n",
    "4. **تبدیل فرمت**: اگر تفاوتی در نام‌گذاری برچسب‌ها وجود دارد (مانند B-PER در دیتاست انگلیسی و B_PER در دیتاست فارسی)، نام‌ها باید استاندارد شوند.\n",
    "\n",
    "5. **ادغام ستون‌ها**: ترکیب ستون‌های tokens و ner_tags از هر دو دیتاست در قالبی واحد که مدل بتواند به راحتی آن را پردازش کند.\n",
    "\n",
    "6. **اعتبارسنجی داده‌ها**: حذف داده‌های ناقص یا نامعتبر پیش از ادغام نهایی.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load datasets\n",
    "persian_dataset = load_dataset(\"AliFartout/PEYMA-ARMAN-Mixed\")\n",
    "english_dataset = load_dataset(\"conll2003\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['B_LOC', 'I_DAT', 'B_PCT', 'I_LOC', 'I_PER', 'I_MON', 'B_ORG', 'B_PRO', 'B_PER', 'O', 'I_PCT', 'I_ORG', 'B_FAC', 'B_DAT', 'B_TIM', 'I_TIM', 'I_EVE', 'B_MON', 'I_PRO', 'B_EVE', 'I_FAC'], id=None), length=-1, id=None), 'ner_tags_names': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n",
      "[5, 20, 1, 20, 20, 20, 20, 1, 20, 20]\n",
      "['B_PER', 'O', 'B_LOC', 'O', 'O', 'O', 'O', 'B_LOC', 'O', 'O']\n",
      "{'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None), 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}\n",
      "[5, 0]\n",
      "['BRUSSELS', '1996-08-22']\n"
     ]
    }
   ],
   "source": [
    "print(persian_dataset['train'].features)\n",
    "# persian_dataset['train']['ner_tags']\n",
    "print(persian_dataset['train']['ner_tags'][0])\n",
    "print(persian_dataset['train']['ner_tags_names'][0])\n",
    "\n",
    "print(english_dataset['train'].features)\n",
    "# persian_dataset['train']['ner_tags']\n",
    "print(english_dataset['train']['ner_tags'][2])\n",
    "print(english_dataset['train']['tokens'][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **سوال اول:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش اول:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: 'B_PER', 20: 'O', 1: 'B_LOC', 12: 'I_PER', 3: 'B_ORG', 10: 'I_ORG', 16: 'B_EVE', 17: 'I_EVE', 8: 'I_LOC', 18: 'B_PRO', 14: 'B_FAC', 15: 'I_FAC', 2: 'B_MON', 9: 'I_MON', 0: 'B_DAT', 7: 'I_DAT', 19: 'I_PRO', 6: 'B_TIM', 13: 'I_TIM', 4: 'B_PCT', 11: 'I_PCT'}\n",
      "{0: 'O', 1: 'B_PER', 2: 'I_PER', 3: 'B_ORG', 4: 'I_ORG', 5: 'B_LOC', 6: 'I_LOC', 7: 'B_MISC', 8: 'I_MISC'}\n"
     ]
    }
   ],
   "source": [
    "id_to_label_mapping_persian = {}\n",
    "for tagsnames, tagslabels in zip(persian_dataset['train']['ner_tags_names'], persian_dataset['train']['ner_tags']):\n",
    "  if tagsnames != None:\n",
    "    for lebel, lebelid in zip(tagsnames, tagslabels):\n",
    "      id_to_label_mapping_persian[lebelid] = lebel\n",
    "\n",
    "\n",
    "id_to_label_mapping_english = {\n",
    "    0: 'O',\n",
    "    1: 'B_PER',\n",
    "    2: 'I_PER',\n",
    "    3: 'B_ORG',\n",
    "    4: 'I_ORG',\n",
    "    5: 'B_LOC',\n",
    "    6: 'I_LOC',\n",
    "    7: 'B_MISC',\n",
    "    8: 'I_MISC'\n",
    "}\n",
    "\n",
    "print(id_to_label_mapping_persian)\n",
    "print(id_to_label_mapping_english)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744a7d2880474b18a349fef0fef6bc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/40425 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80bcaaefac14a7b9a380b5efb8b635f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset has 40425 samples.\n",
      "Test dataset has 6749 samples.\n"
     ]
    }
   ],
   "source": [
    "# General mapping for NER tags\n",
    "general_mapping = {\n",
    "    'O': 0,\n",
    "    'B_PER': 1,\n",
    "    'I_PER': 2,\n",
    "    'B_LOC': 3,\n",
    "    'I_LOC': 4,\n",
    "    'B_ORG': 5,\n",
    "    'I_ORG': 6,\n",
    "    'B_MISC': 7,\n",
    "    'I_MISC': 8,\n",
    "    'B-PER': 1,\n",
    "    'I-PER': 2,\n",
    "    'B-LOC': 3,\n",
    "    'I-LOC': 4,\n",
    "    'B-ORG': 5,\n",
    "    'I-ORG': 6,\n",
    "    'B-MISC': 7,\n",
    "    'I-MISC': 8,\n",
    "    'B_EVE': 9,\n",
    "    'I_EVE': 10,\n",
    "    'B_PRO': 11,\n",
    "    'I_PRO': 12,\n",
    "    'B_FAC': 13,\n",
    "    'I_FAC': 14,\n",
    "    'B_MON': 15,\n",
    "    'I_MON': 16,\n",
    "    'B_DAT': 17,\n",
    "    'I_DAT': 18,\n",
    "    'B_TIM': 19,\n",
    "    'I_TIM': 20,\n",
    "    'B_PCT': 21,\n",
    "    'I_PCT': 22\n",
    "}\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_dataset(dataset, language, split, id_to_label_mapping, general_mapping):\n",
    "    df = pd.DataFrame(dataset[split])\n",
    "\n",
    "    df['language'] = language\n",
    "\n",
    "    # Map `ner_tags` to generalized labels using `id_to_label_mapping` and `general_mapping`\n",
    "    def map_labels(example):\n",
    "        # Convert tag IDs to tag names\n",
    "        tag_names = [id_to_label_mapping[tag] for tag in example]\n",
    "        # Map tag names to the generalized tag space\n",
    "        return [general_mapping[tag] for tag in tag_names if tag in general_mapping]\n",
    "\n",
    "    # Apply mapping to `ner_tags`\n",
    "    df['ner_tags'] = df['ner_tags'].apply(map_labels)\n",
    "\n",
    "    # Drop rows with missing or unprocessable tags (if necessary)\n",
    "    df = df.dropna(subset=['ner_tags']).reset_index(drop=True)\n",
    "\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# Build `id_to_label_mapping` for Persian\n",
    "id_to_label_mapping_persian = {}\n",
    "for tagsnames, tagslabels in zip(persian_dataset['train']['ner_tags_names'], persian_dataset['train']['ner_tags']):\n",
    "    if tagsnames is not None:\n",
    "        for label, label_id in zip(tagsnames, tagslabels):\n",
    "            id_to_label_mapping_persian[label_id] = label\n",
    "\n",
    "# Preprocess Persian and English datasets\n",
    "preprocessed_persian_train = preprocess_dataset(\n",
    "    persian_dataset, language='Persian', split='train',\n",
    "    id_to_label_mapping=id_to_label_mapping_persian, general_mapping=general_mapping\n",
    ")\n",
    "preprocessed_persian_test = preprocess_dataset(\n",
    "    persian_dataset, language='Persian', split='test',\n",
    "    id_to_label_mapping=id_to_label_mapping_persian, general_mapping=general_mapping\n",
    ")\n",
    "preprocessed_english_train = preprocess_dataset(\n",
    "    english_dataset, language='English', split='train',\n",
    "    id_to_label_mapping=id_to_label_mapping_english, general_mapping=general_mapping\n",
    ")\n",
    "preprocessed_english_test = preprocess_dataset(\n",
    "    english_dataset, language='English', split='test',\n",
    "    id_to_label_mapping=id_to_label_mapping_english, general_mapping=general_mapping\n",
    ")\n",
    "\n",
    "# Merge the two datasets\n",
    "merged_train = Dataset.from_pandas(\n",
    "    pd.concat(\n",
    "        [preprocessed_persian_train.to_pandas(), preprocessed_english_train.to_pandas()],\n",
    "        ignore_index=True\n",
    "    )\n",
    ")\n",
    "\n",
    "merged_test = Dataset.from_pandas(\n",
    "    pd.concat(\n",
    "        [preprocessed_persian_test.to_pandas(), preprocessed_english_test.to_pandas()],\n",
    "        ignore_index=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save the merged datasets\n",
    "merged_dataset = DatasetDict({\"train\": merged_train.select_columns([\"tokens\", \"ner_tags\"]), \"test\": merged_test.select_columns([\"tokens\", \"ner_tags\"])})\n",
    "merged_train.save_to_disk(\"./merged_train_dataset\")\n",
    "merged_test.save_to_disk(\"./merged_test_dataset\")\n",
    "\n",
    "print(f\"Train dataset has {len(merged_train)} samples.\")\n",
    "print(f\"Test dataset has {len(merged_test)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_dataset['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px\">\n",
    "\n",
    "*بخش دوم:*\n",
    "---\n",
    "در مسائل Token Classification مانند NER، خروجی مدل‌ها (مانند RobertaForTokenClassification) باید با فرمت ورودی‌ها هماهنگ شود. این هماهنگی یا Alignment به دلایل زیر ضروری است:\n",
    "\n",
    "* وجود Subword Tokenization: مدل‌هایی مثل xlm-Roberta از توکن‌سازی زیرواژه‌ای استفاده می‌کنند، که ممکن است یک کلمه را به چندین زیرواژه تقسیم کند. برای مثال، کلمه‌ای مانند unbelievable ممکن است به [un, ##believable] تبدیل شود. بنابراین برچسب‌دهی باید با این توکن‌های زیرواژه‌ای سازگار شود.\n",
    "\n",
    "* حفظ سازگاری با برچسب‌های اصلی: از آنجا که داده‌ها برچسب‌دهی شده‌اند (مثلاً B_LOC, I_LOC)، باید مطمئن شویم که این برچسب‌ها به درستی به زیرواژه‌ها نگاشت شوند.\n",
    "\n",
    "* افزایش دقت مدل: عدم هماهنگی بین ورودی‌ها و خروجی‌ها می‌تواند باعث خطا و کاهش دقت مدل شود\n",
    "\n",
    "---\n",
    "\n",
    "راه‌حل پیشنهادی:\n",
    "\n",
    "استفاده از Align Labeling: این امکان می‌دهد تا برچسب‌های اصلی (label) را به توکن‌های زیرواژه‌ای مدل تطبیق دهیم.\n",
    "\n",
    "* برای هر توکن اصلی، تطبیق دادن تمامی زیرواژه‌های آن توکن را با برچسب اصلی .\n",
    "\n",
    "* علامت‌گذاری زیرواژه‌هایی که برچسبی ندارند (مانند padding یا special tokens) به عنوان O یا برچسب خنثی .\n",
    "\n",
    "---\n",
    "\n",
    "مراحل آموزش مدل:\n",
    "\n",
    "1. تعریف مدل: استفاده از کلاس RobertaForTokenClassification و مشخص کردن تعداد برچسب‌ها (num_labels).\n",
    "\n",
    "2. آموزش مدل:\n",
    "    * استفاده از داده‌های پردازش‌شده (با Alignment).\n",
    "    * استفاده از یک تابع خطا (مانند cross-entropy) برای محاسبه loss.\n",
    "\n",
    "3. ارزیابی مدل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7027f1267d4fa2ae9144d659bf670d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40425 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70d483a2ce348e2b9f6aa63ea541987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n",
      "Keys in examples: ['tokens', 'ner_tags']\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, TrainingArguments, Trainer, XLMRobertaTokenizerFast\n",
    "from torch import nn\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    print(f\"Keys in examples: {list(examples.keys())}\")\n",
    "\n",
    "    # Tokenize the tokens\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],  # Use \"tokens\" as the input text\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64,\n",
    "        is_split_into_words=True  # Since tokens are provided as words\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):  # Use \"ner_tags\" for labels\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to words\n",
    "        label_ids = []\n",
    "        previous_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)  # Ignored index\n",
    "            elif word_id != previous_word_id:\n",
    "                label_ids.append(label[word_id])  # Use the corresponding label\n",
    "            else:\n",
    "                label_ids.append(-100)  # Ignored for sub-tokens\n",
    "            previous_word_id = word_id\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    # Add the aligned labels to the tokenized inputs\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = merged_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Define the model\n",
    "class XLMRobertaForTokenClassification(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(XLMRobertaForTokenClassification, self).__init__()\n",
    "        self.backbone = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "        self.classifier = nn.Linear(self.backbone.config.hidden_size, num_labels)\n",
    "        self.backbone.config.gradient_checkpointing = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.last_hidden_state)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "num_labels = len(set(label for sublist in merged_dataset[\"train\"][\"ner_tags\"] for label in sublist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = XLMRobertaForTokenClassification(num_labels=num_labels)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Roberta-fa-en-ner\",\n",
    "    log_level=\"error\",\n",
    "    num_train_epochs=1,\n",
    "    # gradient_checkpointing=True,\n",
    "    eval_accumulation_steps=10,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    seed=42,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps=1e6,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=1e6,\n",
    "    eval_steps=400,\n",
    "    push_to_hub=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-9804ac940c57>:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseyyed-msl82\u001b[0m (\u001b[33mseyyed-msl82-university-of-tehran\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241228_045100-seg4j2o3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seyyed-msl82-university-of-tehran/huggingface/runs/seg4j2o3' target=\"_blank\">./Roberta-fa-en-ner</a></strong> to <a href='https://wandb.ai/seyyed-msl82-university-of-tehran/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seyyed-msl82-university-of-tehran/huggingface' target=\"_blank\">https://wandb.ai/seyyed-msl82-university-of-tehran/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seyyed-msl82-university-of-tehran/huggingface/runs/seg4j2o3' target=\"_blank\">https://wandb.ai/seyyed-msl82-university-of-tehran/huggingface/runs/seg4j2o3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 07:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "{'eval_loss': 0.0905294194817543, 'eval_accuracy': 0.9734490681644116, 'eval_precision': 0.9743064873971822, 'eval_recall': 0.9734490681644116, 'eval_f1': 0.9736294743400122, 'eval_runtime': 22.0638, 'eval_samples_per_second': 305.886, 'eval_steps_per_second': 2.402, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (-100) from predictions and labels\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        filtered_labels = []\n",
    "        filtered_predictions = []\n",
    "        for l, p in zip(label, prediction):\n",
    "            if l != -100:\n",
    "                filtered_labels.append(l)\n",
    "                filtered_predictions.append(p)\n",
    "        true_labels.extend(filtered_labels)\n",
    "        true_predictions.extend(filtered_predictions)\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(true_labels, true_predictions, average=\"weighted\")\n",
    "    recall = recall_score(true_labels, true_predictions, average=\"weighted\")\n",
    "    f1 = f1_score(true_labels, true_predictions, average=\"weighted\")\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Add the compute_metrics function to the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px\">\n",
    "\n",
    "*بخش سوم:*\n",
    "---\n",
    "در مسائل مربوط به Token Classification مانند Named Entity Recognition (NER)، متریک Accuracy به تنهایی کافی نیست، زیرا:\n",
    "\n",
    "* عدم توزیع یکنواخت برچسب‌ها: بسیاری از توکن‌ها در جملات معمولاً دارای برچسب خنثی (O) هستند. اگر مدل به طور مداوم فقط برچسب O را پیش‌بینی کند، Accuracy بالایی خواهد داشت، اما عملکرد مدل روی دسته‌های اصلی (مانند B_LOC یا I_PER) ارزیابی نمی‌شود.\n",
    "\n",
    "* بی‌توجهی به تعادل بین Precision و Recall: Accuracy نمی‌تواند تناسب پیش‌بینی‌های صحیح مثبت (True Positives) را با پیش‌بینی‌های غلط (False Positives) یا پیش‌بینی‌های از دست رفته (False Negatives) ارزیابی کند.\n",
    "\n",
    "برای مسائل متعادل‌تر و دقیق‌تر، معیارهای Precision, Recall و F1-Score ضروری هستند:\n",
    "\n",
    "* Precision: دقت در پیش‌بینی برچسب‌های مثبت.\n",
    "\n",
    "* Recall: توانایی مدل در پیدا کردن تمام نمونه‌های مثبت.\n",
    "\n",
    "* F1-Score: میانگین هماهنگ Precision و Recall برای ایجاد توازن بین این د"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "Input Sentence: I am Seyyed Reza Moslemi and love Milan city.\n",
      "Predicted Labels: ['O', 'O', 'B_PER', 'I_PER', 'I_PER', 'I_PER', 'I_PER', 'I_PER', 'I_PER', 'I_PER', 'O', 'O', 'B_LOC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "general_mapping_index_to_id = {\n",
    "    0: 'O',\n",
    "    1: 'B_PER',\n",
    "    2: 'I_PER',\n",
    "    3: 'B_LOC',\n",
    "    4: 'I_LOC',\n",
    "    5: 'B_ORG',\n",
    "    6: 'I_ORG',\n",
    "    7: 'B_MISC',\n",
    "    8: 'I_MISC',\n",
    "    9: 'B_EVE',\n",
    "    10: 'I_EVE',\n",
    "    11: 'B_PRO',\n",
    "    12: 'I_PRO',\n",
    "    13: 'B_FAC',\n",
    "    14: 'I_FAC',\n",
    "    15: 'B_MON',\n",
    "    16: 'I_MON',\n",
    "    17: 'B_DAT',\n",
    "    18: 'I_DAT',\n",
    "    19: 'B_TIM',\n",
    "    20: 'I_TIM',\n",
    "    21: 'B_PCT',\n",
    "    22: 'I_PCT'\n",
    "}\n",
    "\n",
    "# Display output for a sample sentence\n",
    "sample_text = \"I am Seyyed Reza Moslemi and love Milan city.\"  # Example input\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
    "outputs = model(**inputs)\n",
    "predicted_logits = outputs[\"logits\"]\n",
    "predicted_labels = np.argmax(predicted_logits.cpu().detach().numpy(), axis=2)\n",
    "\n",
    "print(len(predicted_labels[0]))\n",
    "\n",
    "# Map predictions back to label names\n",
    "predicted_entities = [general_mapping_index_to_id[label] for label in predicted_labels[0] if label != -100]\n",
    "\n",
    "print(f\"Input Sentence: {sample_text}\")\n",
    "print(f\"Predicted Labels: {predicted_entities[1:-1]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
