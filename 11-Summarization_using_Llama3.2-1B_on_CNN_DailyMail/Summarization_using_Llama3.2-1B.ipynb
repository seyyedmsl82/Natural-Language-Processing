{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*مجموعه داده:*\n",
    "---\n",
    "پیش‌پردازش داده‌ها در حالت کلی الزامی نیست، زیرا این مجموعه داده از قبل آماده‌سازی شده و ویژگی‌های مورد نیاز را شامل می‌شود.\n",
    "دلایل عدم نیاز به پیش‌پردازش سنگین:\n",
    "\n",
    "1. ساختار داده‌های آماده:\n",
    "    * هر نمونه شامل سه ویژگی اصلی است:\n",
    "        article (متن اصلی مقاله).\n",
    "        highlights (خلاصه متن).\n",
    "        id (شناسه یکتا).\n",
    "\n",
    "    * داده‌ها از نظر قالب و محتوا از قبل تمیز و سازمان‌دهی شده‌اند.\n",
    "\n",
    "2. عدم وجود داده‌های نامعتبر یا ناقص:\n",
    "    * هیچ داده نامعتبر (null یا NaN) در مجموعه داده شناسایی نشده است.\n",
    "\n",
    "3. فرمت‌بندی مناسب:\n",
    "    * متن و خلاصه‌ها از نظر ساختار زبانی منسجم هستند و نیاز به تصحیح یا تغییر قالب ندارند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Collecting peft\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
      "Collecting huggingface-hub>=0.25.0 (from peft)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, peft\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.24.7\n",
      "    Uninstalling huggingface-hub-0.24.7:\n",
      "      Successfully uninstalled huggingface-hub-0.24.7\n",
      "Successfully installed huggingface-hub-0.27.0 peft-0.14.0\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install -q datasets\n",
    "!pip install -qqq trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b622098fa7494aa6b2c8297539bd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d39a829679d479fa81e469d3163df41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565d361af565430bba8a41fb5bdc8f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efb46032cdb461387567474658ff0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7325be8c034fb6bf83f1d685db9863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf888a6077c447abfc500e1d191c879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697106b8e8d64b2eb6796168bea7d3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9519653618804419892c29449a0ba3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab22a32f65934b66a2cc5c944feb05a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"cnn_dailymail\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name, \"3.0.0\")\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(dialogue: str, summary: str):\n",
    "    return f\"\"\"### Instruction:\n",
    "Summarize the following conversation.\n",
    "\n",
    "### Input:\n",
    "{dialogue.strip()}\n",
    "\n",
    "### Summary:\n",
    "{summary}\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_instruction_dataset(data_point):\n",
    "\n",
    "    return {\n",
    "        \"article\": data_point[\"article\"],\n",
    "        \"highlights\": data_point[\"highlights\"],\n",
    "        \"text\": format_instruction(data_point[\"article\"],data_point[\"highlights\"])\n",
    "    }\n",
    "\n",
    "def process_dataset(data: Dataset):\n",
    "    return (\n",
    "        data.shuffle(seed=42)\n",
    "        .map(generate_instruction_dataset).remove_columns(['id'])\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55adbd1fe4446d7b6a91e7a1cc496a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf2fc07101e432b81d5a2e26ebb3936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['article', 'highlights', 'text'],\n",
       "     num_rows: 5000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['article', 'highlights', 'text'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['article', 'highlights', 'text'],\n",
       "     num_rows: 500\n",
       " }))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"train\"] = process_dataset(dataset[\"train\"])\n",
    "dataset[\"test\"] = process_dataset(dataset[\"validation\"])\n",
    "dataset[\"validation\"] = process_dataset(dataset[\"validation\"])\n",
    "\n",
    "\n",
    "train_data = dataset['train'].shuffle(seed=42).select([i for i in range(5000)])\n",
    "\n",
    "validation_data = dataset['validation'].shuffle(seed=42).select([i for i in range(500)])\n",
    "test_data = dataset['test'].shuffle(seed=42).select([i for i in range(100)])\n",
    "\n",
    "\n",
    "train_data,test_data,validation_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "{'article': \"Fifty Shades of Grey is set to feature more sex on screen than the 100 raunchiest films released in 2014 put together, making it the most erotic mainstream movie in a decade. The film, which premieres on Valentine's Day features a dozen sex scenes, which make up 20 minutes of its total 100-minute running time. This follows an interview with actor Jamie Dornan, who plays main man Christian Grey, where he says he 'does not believe it is pornographic or even erotic.' Scroll down for video . 'Not even erotic': Jamie Dornan and\\xa0Dakota Johnson star as Christian Grey and Anastasia Steele in the upcoming filmatisation of Fifty Shades of Grey, claimed to be the 'raunchiest' film in a decade . The 'raunchiest of the decade' accolade has been awarded Fifty Shades after website Mr Skin, which ranks films by amount of sex scenes, released the data seen by the Sunday Times. Fifty Shades Of Grey, which stars Dakota Johnson as Anastasia Steele and\\xa0Dornan as the BDSM-loving  Christian Grey, is one of the most eagerly awaited films of the year, and its trailer has already been viewed more than 45 million times on YouTube. But Mrs Taylor-Johnson, 47, said that although the film is true to E L James's best-selling book, it may not be as explicit as people might be hoping for. 'The thing that was most difficult was how and where to pepper the sex, and to not make it feel like it was gratuitous,' she said. 'So it had to be a really strong part of the story, and I had to give characterisation to each sex scene, to make them different. Hot and heavy: Fifty Shades of Grey is set to feature more sex on screen than the 100 raunchiest films released in 2014 put together, making it the most erotic mainstream movie in a decade . Hello Mr Grey: Despite featuring 20 minutes of sex scenes out of the film's total 100-minute running time, neither Dornan nor director Sam Taylor-Johnson considers the film to be erotic . Popular film: The trailer for Fifty Shades has already been viewed more than 45 million times on YouTube . 'I didn't want it to be graphically explicit, and I know that's going to be disappointing to some people,' Mrs Taylor-Johnson, who is married to British actor Aaron Taylor-Johnson, 24, added in an interview with The Guardian newspaper: . 'It's the build up and titillation of touch and sensuality. So I don't think it goes into the realm of porn.' Asked if he felt he had lived up to E L James's fantasies, Dornan, 32, told the Sunday Times Culture supplement: 'Nobody can walk in and embody what she wants. It doesn't exist. 'But I hope I'm the closest thing. I hope I'm a good enough actor, and it's passable.' The father-of-one also said that although the film is sexual, he does not believe it is pornographic or even erotic. 'I just wouldn't use the word 'erotic' - it brings up different ideas for me. I just think we tried to make a good picture, you know?' The series of three Grey books written by James, 51, from London,  has reportedly sold over 100 million copies worldwide and has been translated into 52 languages. Ready for the show: Dakota Johnson was spotted at LAX on Sunday after touching down in Los Angeles . Red room: Dornan has said he hopes that he is 'the closest thing' top how E L James imagines Christian Grey .\", 'highlights': \"A fifth of Fifty Shades of Grey film is made up of sex scenes .\\nHas more sex than the 100 'most naked'\\xa0films of 2014 put together .\\nThe dozen sex scenes makes it the 'raunchiest' film in ten years .\\nDirector Sam Taylor-Johnson has defended film, saying 'it's not porn'\\nLeading\\xa0actor Jamie Dornan has said he 'doesn't believe it's erotic'\", 'text': \"### Instruction:\\nSummarize the following conversation.\\n\\n### Input:\\nFifty Shades of Grey is set to feature more sex on screen than the 100 raunchiest films released in 2014 put together, making it the most erotic mainstream movie in a decade. The film, which premieres on Valentine's Day features a dozen sex scenes, which make up 20 minutes of its total 100-minute running time. This follows an interview with actor Jamie Dornan, who plays main man Christian Grey, where he says he 'does not believe it is pornographic or even erotic.' Scroll down for video . 'Not even erotic': Jamie Dornan and\\xa0Dakota Johnson star as Christian Grey and Anastasia Steele in the upcoming filmatisation of Fifty Shades of Grey, claimed to be the 'raunchiest' film in a decade . The 'raunchiest of the decade' accolade has been awarded Fifty Shades after website Mr Skin, which ranks films by amount of sex scenes, released the data seen by the Sunday Times. Fifty Shades Of Grey, which stars Dakota Johnson as Anastasia Steele and\\xa0Dornan as the BDSM-loving  Christian Grey, is one of the most eagerly awaited films of the year, and its trailer has already been viewed more than 45 million times on YouTube. But Mrs Taylor-Johnson, 47, said that although the film is true to E L James's best-selling book, it may not be as explicit as people might be hoping for. 'The thing that was most difficult was how and where to pepper the sex, and to not make it feel like it was gratuitous,' she said. 'So it had to be a really strong part of the story, and I had to give characterisation to each sex scene, to make them different. Hot and heavy: Fifty Shades of Grey is set to feature more sex on screen than the 100 raunchiest films released in 2014 put together, making it the most erotic mainstream movie in a decade . Hello Mr Grey: Despite featuring 20 minutes of sex scenes out of the film's total 100-minute running time, neither Dornan nor director Sam Taylor-Johnson considers the film to be erotic . Popular film: The trailer for Fifty Shades has already been viewed more than 45 million times on YouTube . 'I didn't want it to be graphically explicit, and I know that's going to be disappointing to some people,' Mrs Taylor-Johnson, who is married to British actor Aaron Taylor-Johnson, 24, added in an interview with The Guardian newspaper: . 'It's the build up and titillation of touch and sensuality. So I don't think it goes into the realm of porn.' Asked if he felt he had lived up to E L James's fantasies, Dornan, 32, told the Sunday Times Culture supplement: 'Nobody can walk in and embody what she wants. It doesn't exist. 'But I hope I'm the closest thing. I hope I'm a good enough actor, and it's passable.' The father-of-one also said that although the film is sexual, he does not believe it is pornographic or even erotic. 'I just wouldn't use the word 'erotic' - it brings up different ideas for me. I just think we tried to make a good picture, you know?' The series of three Grey books written by James, 51, from London,  has reportedly sold over 100 million copies worldwide and has been translated into 52 languages. Ready for the show: Dakota Johnson was spotted at LAX on Sunday after touching down in Los Angeles . Red room: Dornan has said he hopes that he is 'the closest thing' top how E L James imagines Christian Grey .\\n\\n### Summary:\\nA fifth of Fifty Shades of Grey film is made up of sex scenes .\\nHas more sex than the 100 'most naked'\\xa0films of 2014 put together .\\nThe dozen sex scenes makes it the 'raunchiest' film in ten years .\\nDirector Sam Taylor-Johnson has defended film, saying 'it's not porn'\\nLeading\\xa0actor Jamie Dornan has said he 'doesn't believe it's erotic'\"}\n",
      "\n",
      "Longest Article:\n",
      "Washington (CNN) -- Republicans call it a government cover-up similar to what forced Richard Nixon to resign. Democrats call it a right-wing conspiracy theory. The fallout from the September 11, 2012, terrorist attack in Benghazi, Libya, that killed four Americans continues more than 19 months later, with further details last week that raised questions about how the Obama administration responded to the violence less than two months before the President's re-election. Few issues reveal the hyper-partisan politics of Washington more than the ongoing debate over an issue now known simply as Benghazi. Last Friday, House Oversight Committee Chairman Darrell Issa announced that he had subpoenaed Secretary of State John Kerry to testify at a May 21 hearing, alleging that the State Department failed to comply with an earlier subpoena for documents. House Speaker John Boehner followed up by announcing a special congressional committee led by a Republican colleague would investigate the matter. The House voted on party lines Thursday to create the panel, but Democrats have yet to decide if they will take part in what they claim could be a Republican-led witch hunt. GOP-led House votes to establish select committee on Benghazi . Hillary Clinton: No reason for new Benghazi committee . Issa called the administration's lack of compliance \"in violation of any reasonable transparency or historic precedent at least since Richard Milhous Nixon.\" At the White House, spokesman Jay Carney shot back that Republicans continued trying to reap political benefit with what he called conspiracy theories about a Benghazi cover-up. \"What we have seen since hours after the attack, beginning with a statement by the Republican nominee for president, is an attempt by Republicans to politicize a tragedy, and that continues today,\" Carney told reporters, adding that \"what hasn't changed has been the effort by Republicans to ... claim a conspiracy when they haven't been able to find one.\" Here are some answers to questions about the latest twists in the story: . What happened in Benghazi? In September of 2012, a demeaning video made in the United States about the Prophet Mohammed got posted on YouTube and sparked protests at U.S. embassies in the Muslim world. On September 11, the anniversary of the 2001 terrorist attacks on the United States, an assault occurred at a U.S. compound in Benghazi that killed Ambassador Christopher Stevens and three other Americans. The Obama administration initially blamed the Benghazi attack on a protest against the video that escalated into a full-blown tactical assault. As details emerged in ensuing days, it became clear that an al Qaeda-affiliated group took part in what was a coordinated terrorist attack instead of a spontaneous demonstration. Then-Secretary of State Hillary Clinton created a special panel called an Accountability Review Board to investigate what happened. The group led by former Joint Chiefs Chairman Adm. Mike Mullen and retired U.S. Ambassador Thomas Pickering criticized aspects of diplomatic security and made 29 recommendations, all of which were accepted by the State Department. Why all the controversy? Coming less than two months before the presidential election, the Benghazi attack quickly became a political flashpoint. President Barack Obama had campaigned heavily on his decision to approve the mission that killed Osama bin Laden and boasted of putting the al Qaeda leader's organization \"on the run.\" On September 16, five days after the Benghazi attack, then U.S. Ambassador to the United Nations Susan Rice went on Sunday talk shows and said the assault grew out of a protest against the controversial video. Republicans immediately challenged the administration's version of what happened, calling it an attempt by the Obama administration to hide a major security breakdown that signaled the broader failed policy in the region. Obama won re-election in November, but Republicans have mounted congressional investigations into what happened in Benghazi and why Rice gave an incorrect explanation to the American people. What is the Benghazi email everyone is talking about? Last Tuesday, the conservative group Judicial Watch made public State Department documents it received in response to a Freedom of Information Act request. One of the documents was a previously undisclosed email on September 14, 2012, from Ben Rhodes, a national security official specializing in communications, that listed talking points for Rice about the protests that had erupted at U.S. embassies and compounds in the Muslim world. Among the goals listed in the Rhodes email was to \"underscore that these protests are rooted in an Internet video, and not a broader failure of policy.\" Republicans contend the email proves White House manipulation of the messaging for political purposes in the immediate aftermath of Benghazi, despite the administration's contention that Rice relied on talking points provided by the CIA for the sake of uniformity of messaging. \"This is all about an effort to convince the American people that the president of the United States had everything under control,\" GOP Sen. John McCain said. Boehner and other Republicans questioned why the Rhodes email wasn't included in documents that the State Department provided to Congress under the earlier subpoena. In announcing the new subpoena of Kerry, Issa cited what he called \"a disturbing disregard for the (State) Department's legal obligations to Congress.\" Carney argued that the Rhodes email referred to the broader topic of protests throughout the Muslim world, rather than the specific Benghazi attack. Meanwhile, a State Department spokeswoman took issue with Issa's latest subpoena, telling reporters it was a political stunt. Noting Kerry was scheduled to be out of the country on the date of the hearing in the subpoena, Marie Harf said Issa's committee \"would have known if they reached out to us instead of issuing a subpoena.\" Is this new or just more of what we already knew? The existence of the Rhodes email is new, and that provides Republicans with a fresh front in their attacks on the administration over Benghazi. Labeling the situation a \"defiance of the House's subpoena power,\" Boehner called it \"the most flagrant example yet of the administration's contempt for the American people's right to know the truth about what happened when four Americans died in a fiery terrorist attack.\" However, the messaging contained in the Rhodes email is the same as included in previously released documents, such as the CIA talking points that Rice relied on. Carney noted that the only reference in the Rhodes email to Benghazi -- denying that there was actionable intelligence ahead of time of an imminent assault -- was lifted from the CIA talking points. I thought the government promised to release all information? The Obama administration previously pledged to release all pertinent information on Benghazi sought by Congress. Carney noted that it turned over 25,000 pages of documents and that various officials testified at a series of hearings by various congressional committees investigating the matter. Asked why the Rhodes email obtained by Judicial Watch hadn't been turned over previously, Carney said it came under a FOIA request that differed from the congressional subpoenas from Issa's committee. Underlying Carney's explanation was that the Rhodes email referred to the broader issue of protests rather than the specific Benghazi attack, which was the focus of the subpoenas. The talking points supplied by Rhodes were intended to prepare Rice for possible questions in her talk show appearances, he said, calling the document a normal duty of a communications officer in any government. Republicans questioned that explanation, noting the Benghazi attack would clearly be the dominant topic that Rice would face and arguing the administration clearly knew that. What's the upshot of all this? For Republicans, the issue resonates with their conservative base, especially the accusation that the administration failed to provide proper security for American diplomats and was unable to send military assets to respond to the Benghazi attack. At a hearing last week by Issa's committee, a retired Air Force general on duty at U.S. Africa Command that night complained that the military should have tried to save the Benghazi victims even if the effort would have been futile. When Republican Rep. James Lankford asked Brig. Gen. Robert Lovell \"did we have their back that night,\" Lovell responded: \"Obviously not, sir.\" Boehner's announcement of a special panel to further investigate Benghazi fulfilled a request by many GOP colleagues eager to frame the final years of Obama's presidency on their terms. Until now, Boehner had resisted calls to establish such a committee, pointing to the four House panels already investigating the matter. Democrats wanting to get past the issue portray Republicans as driven by partisan desire to hurt Obama. Carney has referred to what he described as GOP conspiracy theories regarding Benghazi that have failed to pan out. \"Everything that this committee would look at has already been looked at ad nauseam by multiple committees,\" Harf said. \"What's the point?\" Is there more than meets the eye? In Washington, always. The issue gives Republicans perhaps their lone line of attack against Hillary Clinton, the overwhelming favorite for the Democratic presidential nomination in 2016 if she decides to run. A new poll Thursday showed Clinton's strong standing. The Quinnipiac University survey from Florida had Clinton topping former two-term Gov. Jeb Bush, the leading potential Republican contender in the nation's most populous swing state. Because Clinton was secretary of state when the Benghazi attack occurred, Republicans have sought to depict her as inattentive to security needs of diplomatic staff. At the Oversight Committee hearing last week, Lovell described how he and others desperately considered possible deployment of a rapid-force team to Benghazi, but needed a State Department request that never came. \"Were they doing what they were trained to do or were they sitting around and waiting for the State Department and Hillary Clinton to call them up and say do something?\" GOP Rep. Jason Chaffetz of Utah asked him. However, the Republican chairman of the House Armed Services Committee, Rep. Howard \"Buck\" McKeon of California, issued a statement that said his panel investigated the matter and found \"no evidence that Department of State officials delayed the decision to deploy what few resources (the military) had available to respond.\" Democrats face what analysts expect will be a difficult mid-term election in November, with little chance of winning back the House from Republicans and facing the possibility of losing their majority in the Senate. Boehner's call for yet another congressional committee to investigate could provide Democrats with a rallying point to motivate voters to prevent Republicans from retaking the Senate in November and gaining full control of Congress. CNN's Jim Acosta and Paul Steinhauser contributed to this report.\n",
      "\n",
      "Shortest Article:\n",
      "(CNN)Each day, CNN producers select a user-submitted photo to be our Travel Photo of the Day. Click through the gallery above to see stunning shots from around the world, and be sure to come back every day for a new image. Have a gorgeous travel photo of your own to share? Submit it for the gallery at CNN iReport!\n",
      "\n",
      "Longest Summary:\n",
      "Rachel Poole had a baby girl via early caesarean section today after she was brutally attacked in her El Paso, Texas, home on Wednesday .\n",
      "Despite her critical condition, she has given her first interview, saying the health of her daughter, Isabella, remains her utmost concern: 'I think the recovery is going pretty well, we have an amazingly strong daughter who is absolutely gorgeous' think the recovery is going pretty well, we have an amazingly strong daughter who is absolutely gorgeous Read More at: http://www.kfoxtv.com/news/features/top-stories/stories/kfox14-speaks-pregnant-woman-knifed-during-video-chat-overseas-husband-2257.shtmlI think the recovery is going pretty well, we have an amazingly strong daughter who is absolutely gorgeous, Read More at: http://www.kfoxtv.com/news/features/top-stories/stories/kfox14-speaks-pregnant-woman-knifed-during-video-chat-overseas-husband-2257.shtmlI think the recovery is going pretty well, we have an amazingly strong daughter who is absolutely gorgeous, Read More at: http://www.kfoxtv.com/news/features/top-stories/stories/kfox14-speaks-pregnant-woman-knifed-during-video-chat-overseas-husband-2257.shtml .\n",
      "She is believed to have been blinded in one eye from stab wounds to the head, suffered a collapsed lung, a displaced vertebrae and has not been able to meet her daughter .\n",
      "New charity fund set up to help the Poole family with their medical costs has received overwhelming support .\n",
      "Rachel's assailant, Corey Bernard Moss, 19, has had his bail raised from $60,000 to $150,000Corey Bernard Moss Read More at: http://www.kfoxtv.com/news/features/top-stories/stories/kfox14-speaks-pregnant-woman-knifed-during-video-chat-overseas-husband-2257.shtml .\n",
      "\n",
      "Shortest Summary:\n",
      "TV chef is going back to his Italian roots .\n",
      "Mean Article Length: 4044.66 characters\n",
      "Mean Summary Length: 294.21 characters\n"
     ]
    }
   ],
   "source": [
    "# Analyze and print one sample\n",
    "print(\"Sample data:\")\n",
    "print(train_data[0])\n",
    "\n",
    "# Find longest and shortest articles and summaries\n",
    "longest_text = max(train_data, key=lambda x: len(x[\"article\"]))\n",
    "shortest_text = min(train_data, key=lambda x: len(x[\"article\"]))\n",
    "longest_summary = max(train_data, key=lambda x: len(x[\"highlights\"]))\n",
    "shortest_summary = min(train_data, key=lambda x: len(x[\"highlights\"]))\n",
    "\n",
    "print(\"\\nLongest Article:\")\n",
    "print(longest_text[\"article\"])\n",
    "print(\"\\nShortest Article:\")\n",
    "print(shortest_text[\"article\"])\n",
    "print(\"\\nLongest Summary:\")\n",
    "print(longest_summary[\"highlights\"])\n",
    "print(\"\\nShortest Summary:\")\n",
    "print(shortest_summary[\"highlights\"])\n",
    "\n",
    "\n",
    "# Calculate the mean length of articles\n",
    "mean_article_length = sum(len(x[\"article\"]) for x in train_data) / len(train_data)\n",
    "\n",
    "# Calculate the mean length of summaries\n",
    "mean_summary_length = sum(len(x[\"highlights\"]) for x in train_data) / len(train_data)\n",
    "\n",
    "print(f\"Mean Article Length: {mean_article_length:.2f} characters\")\n",
    "print(f\"Mean Summary Length: {mean_summary_length:.2f} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"MY TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5395a2622b44638fe02e322d515431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904c33c6b5fa43c88e19927dd41d9388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce11591fc8444ab9d2f8a4af3f3f022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353520a3f80242899e9c01e6f0264eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21d4b5a8b844a66b8cb323e38bc956f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5d3657689643d6ab1d13c92a36cdb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config, \n",
    "    device_map=\"auto\", \n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "\n",
    "*بخش اول:*\n",
    "---\n",
    "\n",
    "مقایسه نتایج Zero-Shot و Few-Shot\n",
    "* Zero-Shot\n",
    "\n",
    "    * ROUGE:\n",
    "        rouge1: 0.1036، rouge2: 0.0693، rougeL: 0.0950\n",
    "\n",
    "    * BERTScore:\n",
    "        Precision: 0.7927، Recall: 0.8946، F1: 0.8406\n",
    "\n",
    "* Few-Shot\n",
    "\n",
    "    * ROUGE:\n",
    "        rouge1: 0.0276، rouge2: 0.0021، rougeL: 0.0191\n",
    "\n",
    "    * BERTScore:\n",
    "        Precision: 0.7518، Recall: 0.7957، F1: 0.7731\n",
    "\n",
    "* تحلیل\n",
    "\n",
    "    * Zero-Shot بهتر عمل می‌کند:\n",
    "        در هر دو معیار ROUGE و BERTScore، Zero-Shot عملکرد بهتری دارد.\n",
    "        \n",
    "        Recall و F1 بالاتر نشان‌دهنده همخوانی بهتر با معنای متن مرجع است.\n",
    "\n",
    "    * Few-Shot ضعیف‌تر است:\n",
    "        \n",
    "        ROUGE بسیار پایین‌تر است، نشان‌دهنده مشکل در تولید خلاصه‌های مشابه متن مرجع.\n",
    "        \n",
    "        Precision پایین‌تر نشان‌دهنده محتوای غیرمرتبط بیشتر است.\n",
    "\n",
    "---\n",
    "\n",
    "مفاهیم:\n",
    "1. چالش‌های ارزیابی مدل‌های زبانی در تولید متن خلاصه‌سازی\n",
    "\n",
    "    * چالش‌ها:\n",
    "\n",
    "        ذهنی بودن معیارها: ارزیابی خلاصه‌سازی تا حدی ذهنی است، زیرا یک متن خلاصه می‌تواند به روش‌های مختلف صحیح باشد.\n",
    "\n",
    "        عدم تطابق معیارها با کیفیت انسانی: معیارهایی مثل ROUGE بیشتر بر شباهت لغوی تکیه دارند و ممکن است خلاصه‌های معنایی درست را کم‌ارزش کنند.\n",
    "        \n",
    "        تفاوت سبک نگارشی: خلاصه‌های تولیدی ممکن است سبک متفاوتی نسبت به مرجع داشته باشند، حتی اگر معنایی مشابه ارائه دهند.\n",
    "\n",
    "2. توضیح معیارهای ROUGE و BERTScore\n",
    "\n",
    "    * ROUGE:\n",
    "    \n",
    "        مزایا: بر اساس همپوشانی n-gram بین خلاصه تولیدشده و مرجع عمل می‌کند و در مقایسه خلاصه‌های لغوی مناسب است.\n",
    "        معایب: به خلاصه‌هایی که بازنویسی شده یا سبک متفاوتی دارند، امتیاز پایینی می‌دهد.\n",
    "    \n",
    "    * BERTScore:\n",
    "    \n",
    "        مزایا: به جای شباهت لغوی، از مدل‌های زبان بزرگ برای مقایسه معنایی خلاصه استفاده می‌کند.\n",
    "        معایب: نیازمند منابع محاسباتی بیشتر بوده و گاهی به خروجی‌های کلی امتیاز بالا می‌دهد.\n",
    "\n",
    "3. تأثیر کوانتایزر 4 بیتی (Quantization)\n",
    "\n",
    "    * علت استفاده: کاهش حافظه مورد نیاز و افزایش سرعت در پردازش مدل‌ها.\n",
    "    \n",
    "    * تأثیر:\n",
    "    \n",
    "        سرعت: پردازش سریع‌تر در محیط‌هایی با منابع محدود.\n",
    "        دقت: ممکن است باعث افت جزیی در دقت مدل شود، به خصوص در وظایف حساس به جزئیات.\n",
    "\n",
    "4. انتخاب نمونه‌های ورودی در Few-Shot\n",
    "\n",
    "    * معیارها:\n",
    "    \n",
    "        انتخاب نمونه‌هایی که پوشش‌دهی خوبی از موضوعات و سبک‌های مختلف متن داشته باشند.\n",
    "    \n",
    "        انتخاب مثال‌های ساده و واضح برای راهنمایی بهتر مدل.\n",
    "    \n",
    "    * تأثیر: نمونه‌های مناسب می‌توانند عملکرد مدل را در خلاصه‌سازی بهبود بخشند.\n",
    "\n",
    "5. مقایسه نسخه ساده و نسخه Instruct-Tune\n",
    "\n",
    "    * نسخه ساده:\n",
    "    \n",
    "        بدون آموزش اضافی و مستقیم از مدل پیش‌آموزش‌دیده استفاده می‌کند.\n",
    "    \n",
    "        مزیت: سریع و ساده.\n",
    "    \n",
    "        معایب: عملکرد ضعیف‌تر در وظایف خاص.\n",
    "    \n",
    "    * نسخه Instruct-Tune:\n",
    "    \n",
    "        با تنظیم مدل روی داده‌های خاص، عملکرد بهتری در وظایف مرتبط مانند خلاصه‌سازی دارد.\n",
    "    \n",
    "        مزیت: دقت و کیفیت بالاتر.\n",
    "    \n",
    "        معایب: نیاز به منابع بیشتر برای آموزش."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Zero-Shot Summarization ===\n",
      "Input Article:\n",
      "A £1billion tidal lagoon off the coast of south Wales will form the centrepiece of ambitious renewable energy plans to be unveiled today. George Osborne is set to use his Budget today to announce that the Government is beginning formal talks on funding the project to produce energy from turbines in Swansea Bay, south Wales. The scheme would be over an area of more than four square miles cordoned off by a breakwater, with power generated as the tides pass through turbines when going in and out. Scroll down for video . Construction: The project includes creating a six-mile sea reef, the reintroduction of the native oyster to Swansea Bay and an offshore visitor centre as well as national triathlon and water sports facilities . Below the surface: This graphic shows how the structure would generate energy at high tide, left, and low tide, right, meaning that water will flow through four times daily . Vast: The scheme would be over an area of more than four square miles of Swansea Bay in South Wales, creating a large tidal lagoon cordoned off by a huge breakwater . Artist's impression: Chancellor George Osborne is set to use his Budget today to announce a £1billion plan for the world's first tidal lagoon to generate electricity from turbines in Swansea Bay, south Wales . The Tidal Lagoon Power company will get the opportunity to discuss subsidies - which are expected to have to be about £150 per megawatt hour, reported The Guardian. Controversially, this is far greater than the subsidy of up to £92.50 per MWh agreed for the planned Somerset nuclear plant Hinkley Point C - and triple the wholesale electricity prices of £50 per MWh. But the Department of Energy and Climate Change is said to have been reassured that if a prototype can be built in Wales, it will be possible to launch the concept more cheaply elsewhere. The Swansea Bay scheme would need a six-mile sea wall up to 65ft high - although only half of this would be noticeable from land at low tide, according to Tidal Lagoon Power. Budget announcement: Mr Osborne (file picture) is expected to reveal that the Government is beginning formal talks on funding the project to produce energy from turbines in Swansea Bay . Other project: The Tidal Lagoon Power company will get the opportunity to discuss subsidies - which are expected to have to be about £150 per megawatt hour. Controversially, this is far greater than the £98 per MWh subsidy agreed for the planned Somerset nuclear plant Hinkley Point C (above, artist's impression) The wall would create a lagoon in the Severn Estuary with turbines that could harness the incoming and outgoing tides to generate power 14 hours a day. The developers say the  project, along with four others, could meet 10 per cent of our electricity needs from the tides by 2023. The six-mile wall around Swansea Bay complete with turbines could generate electricity 14 hours a day. Tidal Lagoon Power believes the project will boost a UK supply chain and create a new export market if it gets the go-ahead. The project includes creating a six-mile sea reef, the reintroduction of the native oyster to Swansea Bay and a visitor centre. There are claims that it would save 236,000 tonnes of carbon a year, as well as create 1,850 construction jobs and 150 long-term jobs in its operation. When plans were first submitted last year, it was claimed that the ground-breaking project could provide renewable power for 120,000 homes in Swansea for 120 years. Developers behind the project believe it could be the first step in developing lagoon technology that could meet 10 per cent of the UK's electricity needs from the tides by 2023. Tidal Lagoon Power said it hopes that 65 per cent of expenditure will be in the UK, boosting a home grown supply chain and creating a possible future export market. The project includes creating a six-mile sea reef, the reintroduction of the native oyster to Swansea Bay and an offshore visitor centre as well as national triathlon and water sports facilities. It is claimed that building more, larger lagoons would bring economies of scale. Other sites where lagoons are being considered, such as the Somerset coast, could have added benefits such as flood defences. Last month it was reported that the plan had been given a funding boost after investment management firm InfraRed Capital Partners agreed to provide £100million for the project. But, also last month, consumer charity Citizens Advice said the project was ‘appalling value for money’ and should not receive subsidies.\n",
      "\n",
      "Generated Summary:\n",
      "A £1billion tidal lagoon off the coast of south Wales will form the centrepiece of ambitious renewable energy plans to be unveiled today. George Osborne is set to use his Budget today to announce that the Government is beginning formal talks on funding the project to produce energy from turbines in Swansea Bay, south Wales. The scheme would be over an area of more than four square miles cordoned off by a breakwater, with power generated as the tides pass through turbines when going in and out. Scroll down for video. Construction: The project includes creating a six-mile sea reef, the reintroduction of the native oyster to Swansea Bay and an offshore visitor centre as well as national triathlon and water sports facilities. Below the surface: This graphic shows how the structure would generate energy at high tide, left, and low tide, right, meaning that water will flow through four times daily. Vast: The scheme would be over an area of more than four square miles of Swansea Bay in South Wales, creating a large tidal lagoon cordoned off by a huge breakwater. Artist's impression: Chancellor George Osborne is set to use his Budget today to announce a £1billion plan for the world's first tidal lagoon to generate electricity from turbines in Swansea Bay, south Wales. The Tidal Lagoon Power company will get the opportunity to discuss subsidies - which are expected to have to be about £150 per megawatt hour, reported The Guardian. Controversially, this is far greater than the subsidy of up to £92.50 per MWh agreed for the planned Somerset nuclear plant Hinkley Point C - and triple the wholesale electricity prices of £50 per MWh. But the Department of Energy and Climate Change is said to have been reassured that if a prototype can be built in Wales, it will be possible to launch the concept more cheaply elsewhere. The Swansea Bay scheme would need a six-mile sea wall up to 65ft high - although only half of this would be noticeable from land at low tide, according to Tidal Lagoon Power. Budget announcement: Mr Osborne (file picture) is expected to reveal that the Government is beginning formal talks on funding the project to produce energy from turbines in Swansea Bay. Other project: The Tidal Lagoon Power company will get the opportunity to discuss subsidies - which are expected to have to be about £150 per megawatt hour. Controversially, this is far greater than the £98 per MWh subsidy agreed for the planned Somerset nuclear plant Hinkley Point C - and triple the wholesale electricity prices of £50 per MWh. But the Department of Energy and Climate Change is said to have been reassured that if a prototype can be built in Wales, it will be possible to launch the concept more cheaply elsewhere. The Swansea Bay scheme would need a six-mile sea wall up to 65ft high - although only half of this would be noticeable from land at low tide, according to Tidal Lagoon Power. Budget announcement: Mr Osborne (file picture) is expected to reveal that the Government is beginning formal talks on funding the project to produce energy from turbines in Swansea Bay. Other project: The Tidal Lagoon Power company will get the opportunity to discuss subsidies - which are expected to have to be about £150 per megawatt hour. Controversially, this is far greater than the £98 per MWh subsidy agreed for the planned Somerset nuclear plant Hinkley Point C - and triple the wholesale electricity prices of £50 per MWh. But the Department of Energy and Climate Change is said to have been reassured that if a prototype can be built in Wales, it will be possible to launch the concept more cheaply elsewhere. The Swansea Bay scheme would need a six-mile sea wall up to 65ft high - although only half of this would be noticeable from land at low tide, according to Tidal Lagoon Power. Budget announcement: Mr Osborne (\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zero-Shot Summarization\n",
    "def zero_shot_summarization(test_data, model, tokenizer):\n",
    "    # Select a single sample from the test dataset\n",
    "    sample = test_data[0]\n",
    "    input_text = sample[\"article\"]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    ).to(\"cuda\")  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=294,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"\\n=== Zero-Shot Summarization ===\")\n",
    "    print(f\"Input Article:\\n{input_text}\\n\")\n",
    "    print(f\"Generated Summary:\\n{generated_summary}\\n\")\n",
    "\n",
    "    return input_text, generated_summary\n",
    "\n",
    "article, generated_summary = zero_shot_summarization(test_data, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Few-Shot Summarization ===\n",
      "Few-Shot Prompt:\n",
      "### Few-Shot Examples:\n",
      "Article: Fifty Shades of Grey is set to feature more sex on screen than the 100 raunchiest films released in 2014 put together, making it the most erotic mainstream movie in a decade. The film, which premieres on Valentine's Day features a dozen sex scenes, which make up 20 minutes of its total 100-minute running time. This follows an interview with actor Jamie Dornan, who plays main man Christian Grey, where he says he 'does not believe it is pornographic or even erotic.' Scroll down for video . 'Not even erotic': Jamie Dornan and Dakota Johnson star as Christian Grey and Anastasia Steele in the upcoming filmatisation of Fifty Shades of Grey, claimed to be the 'raunchiest' film in a decade . The 'raunchiest of the decade' accolade has been awarded Fifty Shades after website Mr Skin, which ranks films by amount of sex scenes, released the data seen by the Sunday Times. Fifty Shades Of Grey, which stars Dakota Johnson as Anastasia Steele and Dornan as the BDSM-loving  Christian Grey, is one of the most eagerly awaited films of the year, and its trailer has already been viewed more than 45 million times on YouTube. But Mrs Taylor-Johnson, 47, said that although the film is true to E L James's best-selling book, it may not be as explicit as people might be hoping for. 'The thing that was most difficult was how and where to pepper the sex, and to not make it feel like it was gratuitous,' she said. 'So it had to be a really strong part of the story, and I had to give characterisation to each sex scene, to make them different. Hot and heavy: Fifty Shades of Grey is set to feature more sex on screen than the 100 raunchiest films released in 2014 put together, making it the most erotic mainstream movie in a decade . Hello Mr Grey: Despite featuring 20 minutes of sex scenes out of the film's total 100-minute running time, neither Dornan nor director Sam Taylor-Johnson considers the film to be erotic . Popular film: The trailer for Fifty Shades has already been viewed more than 45 million times on YouTube . 'I didn't want it to be graphically explicit, and I know that's going to be disappointing to some people,' Mrs Taylor-Johnson, who is married to British actor Aaron Taylor-Johnson, 24, added in an interview with The Guardian newspaper: . 'It's the build up and titillation of touch and sensuality. So I don't think it goes into the realm of porn.' Asked if he felt he had lived up to E L James's fantasies, Dornan, 32, told the Sunday Times Culture supplement: 'Nobody can walk in and embody what she wants. It doesn't exist. 'But I hope I'm the closest thing. I hope I'm a good enough actor, and it's passable.' The father-of-one also said that although the film is sexual, he does not believe it is pornographic or even erotic. 'I just wouldn't use the word 'erotic' - it brings up different ideas for me. I just think we tried to make a good picture, you know?' The series of three Grey books written by James, 51, from London,  has reportedly sold over 100 million copies worldwide and has been translated into 52 languages. Ready for the show: Dakota Johnson was spotted at LAX on Sunday after touching down in Los Angeles . Red room: Dornan has said he hopes that he is 'the closest thing' top how E L James imagines Christian Grey .\n",
      "Summary: A fifth of Fifty Shades of Grey film is made up of sex scenes .\n",
      "Has more sex than the 100 'most naked' films of 2014 put together .\n",
      "The dozen sex scenes makes it the 'raunchiest' film in ten years .\n",
      "Director Sam Taylor-Johnson has defended film, saying 'it's not porn'\n",
      "Leading actor Jamie Dornan has said he 'doesn't believe it's erotic'\n",
      "\n",
      "Article: (CNN) -- A fourth person has died, a day after a lumber plant employee shot several people at his company's factory near Lucerne, Switzerland. The shooter killed himself and two other people Wednesday, and one of seven other people wounded in the incident died Thursday morning, according to Lucerne county police spokesman Urs Wigger. Police have not yet named any of the victims, and they had no update Thursday on how the other six people were faring. The murder weapon was a Sphinx AT-380 handgun -- not a military-issued weapon -- Wigger said, adding that police were investigating how the shooter got the gun, and whether he had it legally. The gunman, 42, was a long-time employee of Kronospan, a wood panel manufacturer in the village of Menznau, about 30 kilometers (19 miles) outside Lucerne, police said. The factory employs 350 people, according to a Lucerne-based newspaper. It's unclear why the gunman opened fire as employees were taking a morning break. Local media reports suggest the plant has had to cut production because of a reduction in the timber harvest. A statement posted in five languages on Kronospan's Swiss website on Wednesday read: \"It is with tremendous sadness that we must confirm that there has been a shooting incident at our factory this morning, resulting in multiple serious injuries and fatalities. The shock and grief at Kronospan is not possible to put into words.\" The posting also included a hotline number for employees. That hotline has been \"widely used,\" according to police. Representatives for the company's U.S. and UK branches said they had limited information about what happened and offered their condolences. \"It's a terrible tragedy, and our thoughts are with the families of the people who have lost their lives,\" the UK spokesman said. \"We, too, are deeply saddened by this tragic event, and our thoughts are with our Swiss colleagues and the families of those involved,\" said the U.S. spokeswoman. The Menznau Kronospan factory, which makes wood products under the brand name Kronoswiss, is one of 10 plants in seven countries run by the Swiss Krono Group, according to the company's website. Its U.S. subsidiary, Kronotex USA, says on its website that the company has been a family business since its inception in 1897 in Austria, and its total annual sales exceed $1 billion. CNN's Fred Pleitgen, Saskya Vandoorne and Carol Jordan contributed to this report. CNN's Laura Spark-Smith wrote from London, and Mark Morgenstein and Ben Brumfield wrote from Atlanta.\n",
      "Summary: NEW:  A fourth person has died as a result of a shooting at a Swiss wood company .\n",
      "NEW: The weapon used was not military-issued, and it's unclear if it was legally owned .\n",
      "NEW: Gunfire wounded six other people at the factory .\n",
      "The company, which makes wood products, employs 350 people at the crime scene .\n",
      "\n",
      "Article: How did the heron cross the river? On its hippo friend's back of course! A visitor at the Kruger National Park in South Africa filmed the unlikely duo in action. The long-legged bird is seen surfing downstream, while its large pal paddles underwater. At one point, the hippo raises its head for air. In then dips down again, to keep the ship sailing. The heron doesn't move a muscle as it is chauffeured along. To date the video of the 'surfing' bird has been watched more than 34,000 times. Some viewers have said that they've seen similar river cruising antics at the park. How did the heron cross the river? On its hippo friend's back of course! Caught on camera: A visitor at the Kruger National Park in South Africa filmed the unlikely duo in action . Tootling along: The long-legged bird is seen surfing downstream, while its large pal paddles underwater . Exhausting work: At one point, the hippo raises its head for air .\n",
      "Summary: A visitor at the Kruger National Park in South Africa filmed the unlikely duo in action .\n",
      "\n",
      "### Target Article:\n",
      "A £1billion tidal lagoon off the coast of south Wales will form the centrepiece of ambitious renewable energy plans to be unveiled today. George Osborne is set to use his Budget today to announce that the Government is beginning formal talks on funding the project to produce energy from turbines in Swansea Bay, south Wales. The scheme would be over an area of more than four square miles cordoned off by a breakwater, with power generated as the tides pass through turbines when going in and out. Scroll down for video . Construction: The project includes creating a six-mile sea reef, the reintroduction of the native oyster to Swansea Bay and an offshore visitor centre as well as national triathlon and water sports facilities . Below the surface: This graphic shows how the structure would generate energy at high tide, left, and low tide, right, meaning that water will flow through four times daily . Vast: The scheme would be over an area of more than four square miles of Swansea Bay in South Wales, creating a large tidal lagoon cordoned off by a huge breakwater . Artist's impression: Chancellor George Osborne is set to use his Budget today to announce a £1billion plan for the world's first tidal lagoon to generate electricity from turbines in Swansea Bay, south Wales . The Tidal Lagoon Power company will get the opportunity to discuss subsidies - which are expected to have to be about £150 per megawatt hour, reported The Guardian. Controversially, this is far greater than the subsidy of up to £92.50 per MWh agreed for the planned Somerset nuclear plant Hinkley Point C - and triple the wholesale electricity prices of £50 per MWh. But the Department of Energy and Climate Change is said to have been reassured that if a prototype can be built in Wales, it will be possible to launch the concept more cheaply elsewhere. The Swansea Bay scheme would need a six-mile sea wall up to 65ft high - although only half of this would be noticeable from land at low tide, according to Tidal Lagoon Power. Budget announcement: Mr Osborne (file picture) is expected to reveal that the Government is beginning formal talks on funding the project to produce energy from turbines in Swansea Bay . Other project: The Tidal Lagoon Power company will get the opportunity to discuss subsidies - which are expected to have to be about £150 per megawatt hour. Controversially, this is far greater than the £98 per MWh subsidy agreed for the planned Somerset nuclear plant Hinkley Point C (above, artist's impression) The wall would create a lagoon in the Severn Estuary with turbines that could harness the incoming and outgoing tides to generate power 14 hours a day. The developers say the  project, along with four others, could meet 10 per cent of our electricity needs from the tides by 2023. The six-mile wall around Swansea Bay complete with turbines could generate electricity 14 hours a day. Tidal Lagoon Power believes the project will boost a UK supply chain and create a new export market if it gets the go-ahead. The project includes creating a six-mile sea reef, the reintroduction of the native oyster to Swansea Bay and a visitor centre. There are claims that it would save 236,000 tonnes of carbon a year, as well as create 1,850 construction jobs and 150 long-term jobs in its operation. When plans were first submitted last year, it was claimed that the ground-breaking project could provide renewable power for 120,000 homes in Swansea for 120 years. Developers behind the project believe it could be the first step in developing lagoon technology that could meet 10 per cent of the UK's electricity needs from the tides by 2023. Tidal Lagoon Power said it hopes that 65 per cent of expenditure will be in the UK, boosting a home grown supply chain and creating a possible future export market. The project includes creating a six-mile sea reef, the reintroduction of the native oyster to Swansea Bay and an offshore visitor centre as well as national triathlon and water sports facilities. It is claimed that building more, larger lagoons would bring economies of scale. Other sites where lagoons are being considered, such as the Somerset coast, could have added benefits such as flood defences. Last month it was reported that the plan had been given a funding boost after investment management firm InfraRed Capital Partners agreed to provide £100million for the project. But, also last month, consumer charity Citizens Advice said the project was ‘appalling value for money’ and should not receive subsidies.\n",
      "\n",
      "### Predicted Summary:\n",
      "\n",
      "\n",
      "Generated Summary:\n",
      "### Few-Shot Examples:\n",
      "Article: Fifty Shades of Grey is set to feature more sex on screen than the 100 raunchiest films released in 2014 put together, making it the most erotic mainstream movie in a decade. The film, which premieres on Valentine's Day features a dozen sex scenes, which make up 20 minutes of its total 100-minute running time. This follows an interview with actor Jamie Dornan, who plays main man Christian Grey, where he says he 'does not believe it is pornographic or even erotic.' Scroll down for video. 'Not even erotic': Jamie Dornan and Dakota Johnson star as Christian Grey and Anastasia Steele in the upcoming filmatisation of Fifty Shades of Grey, claimed to be the 'raunchiest' film in a decade. The 'raunchiest of the decade' accolade has been awarded Fifty Shades after website Mr Skin, which ranks films by amount of sex scenes, released the data seen by the Sunday Times. Fifty Shades Of Grey, which stars Dakota Johnson as Anastasia Steele and Dornan as the BDSM-loving  Christian Grey, is one of the most eagerly awaited films of the year, and its trailer has already been viewed more than 45 million times on YouTube. But Mrs Taylor-Johnson, 47, said that although the film is true to E L James's best-selling book, it may not be as explicit as people might be hoping for. 'The thing that was most difficult was how and where to pepper the sex, and to not make it feel like it was gratuitous,' she said. 'So it had to be a really strong part of the story, and I had to give characterisation to each sex scene, to make them different. Hot and heavy: Fifty Shades of Grey is set to feature more sex on screen than the 100 raunchiest films released in 2014 put together, making it the most erotic mainstream movie in a decade. Hello Mr Grey: Despite featuring 20 minutes of sex scenes out of the film's total 100-minute running time, neither Dornan nor director Sam Taylor-Johnson considers the film to be erotic. Popular film: The trailer for Fifty Shades has already been viewed more than 45 million times on YouTube. 'I didn't want it to be graphically explicit, and I know that's going to be disappointing to some people,' Mrs Taylor-Johnson, who is married to British actor Aaron Taylor-Johnson, 24, added in an interview with The Guardian newspaper:. 'It's the build up and titillation of touch and sensuality. So I don't think it goes into the realm of porn.' Asked if he felt he had lived up to E L James's fantasies, Dornan, 32, told the Sunday Times Culture supplement: 'Nobody can walk in and embody what she wants. It doesn't exist. 'But I hope I'm the closest thing. I hope I'm a good enough actor, and it's passable.' The father-of-one also said that although the film is sexual, he does not believe it is pornographic or even erotic. 'I just wouldn't use the word 'erotic' - it brings up different ideas for me. I just think we tried to make a good picture, you know?' The series of three Grey books written by James, 51, from London,  has reportedly sold over 100 million copies worldwide and has been translated into 52 languages. Ready for the show: Dakota Johnson was spotted at LAX on Sunday after touching down in Los Angeles. Red room: Dornan has said he hopes that he is 'the closest thing' top how E L James imagines Christian Grey.\n",
      "Summary: A fifth of Fifty Shades of Grey film is made up of sex scenes.\n",
      "Has more sex than the 100'most naked' films of 2014 put together.\n",
      "The dozen sex scenes makes it the 'raunchiest' film in ten years.\n",
      "Director Sam Taylor-Johnson has defended film, saying 'it's not porn'\n",
      "Leading actor Jamie Dornan has said he 'doesn't believe it's erotic'\n",
      "\n",
      "Article: (CNN) -- A fourth person has died, a day after a lumber plant employee shot several people at his company's factory near Lucerne, Switzerland. The shooter killed himself and two other people Wednesday, and one of seven other people wounded in the incident died Thursday morning, according to Lucerne county police spokesman Urs Wigger. Police have not yet named any of the victims, and they had no update Thursday on how the other six people were faring. The murder weapon was a Sphinx AT-380 handgun -- not a military-issued weapon -- Wigger said, adding that police were investigating how the shooter got the gun, and whether he had it legally. The gunman, 42, was a long-time employee of Kronospan, a wood panel manufacturer in the village of Menznau, about 30 kilometers (19 miles) outside Lucerne. He had been working at Kronospan for over 20 years, and was known for his friendly demeanor. The shooting occurred at the Kronospan factory in Menznau, a small village in the canton of Lucerne, about 30 kilometers (19 miles) outside Lucerne. The shooting occurred at the Kronospan factory in Menznau, a small village in the canton of Lucerne, about 30 kilometers (19 miles) outside Lucerne. The shooting occurred at the Kronospan factory in Menznau, a small village in the canton of Lucerne, about 30 kilometers (19 miles) outside Lucerne. The shooting occurred at the Kronospan factory in Menznau, a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Few-Shot Summarization\n",
    "def few_shot_summarization(train_data, test_data, model, tokenizer):\n",
    "    # Select three few-shot examples from the training data\n",
    "    few_shot_examples = train_data.select([0, 1, 2])\n",
    "    input_text = test_data[0][\"article\"]\n",
    "\n",
    "\n",
    "    prompt = \"### Few-Shot Examples:\\n\"\n",
    "    for example in few_shot_examples:\n",
    "        prompt += (\n",
    "            f\"Article: {example['article']}\\n\"\n",
    "            f\"Summary: {example['highlights']}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    prompt += f\"### Target Article:\\n{input_text}\\n\\n### Predicted Summary:\\n\"\n",
    "\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=1024\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=150,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "\n",
    "    generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"\\n=== Few-Shot Summarization ===\")\n",
    "    print(f\"Few-Shot Prompt:\\n{prompt}\\n\")\n",
    "    print(f\"Generated Summary:\\n{generated_summary}\\n\")\n",
    "\n",
    "    return input_text, generated_summary\n",
    "\n",
    "\n",
    "article, generated_summary_fewshot = few_shot_summarization(train_data, test_data, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.4.1+cu121)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.1.4)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.47.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.27.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=3a84c7e5169d396d55ea7b29f52c5c49eb66dc28525b75765f5a600f181be137\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score, bert_score\n",
      "Successfully installed bert_score-0.3.13 rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score bert_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation for Zero-Shot Summarization ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9915fd176d70419485fb5c465a006b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31467dbfd685409c988ca32d9520549a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e594b0b13e7c44ec867d6af8931d1755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9793ca2252f9445cb47bdc8b85db2ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8b945dd9034c33992d682564b4ca9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe125209f504ddda6d5036d492d6bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot ROUGE Scores: {'rouge1': 0.10359712230215828, 'rouge2': 0.06926406926406926, 'rougeL': 0.0949640287769784}\n",
      "Zero-Shot BERTScore: {'precision': 0.7927143573760986, 'recall': 0.8945730924606323, 'f1': 0.840569257736206}\n",
      "\n",
      "=== Evaluation for Few-Shot Summarization ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot ROUGE Scores: {'rouge1': 0.027571580063626724, 'rouge2': 0.0021253985122210418, 'rougeL': 0.019088016967126194}\n",
      "Few-Shot BERTScore: {'precision': 0.7517890930175781, 'recall': 0.7956898808479309, 'f1': 0.7731167674064636}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "\n",
    "def evaluate_with_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    results = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        results[\"rouge1\"].append(scores[\"rouge1\"].fmeasure)\n",
    "        results[\"rouge2\"].append(scores[\"rouge2\"].fmeasure)\n",
    "        results[\"rougeL\"].append(scores[\"rougeL\"].fmeasure)\n",
    "    \n",
    "    # Calculate average ROUGE scores\n",
    "    avg_results = {metric: sum(scores) / len(scores) for metric, scores in results.items()}\n",
    "    return avg_results\n",
    "\n",
    "def evaluate_with_bertscore(predictions, references, lang=\"en\"):\n",
    "    P, R, F1 = score(predictions, references, lang=lang)\n",
    "    return {\"precision\": P.mean().item(), \"recall\": R.mean().item(), \"f1\": F1.mean().item()}\n",
    "\n",
    "# Evaluate Zero-Shot Summarization\n",
    "print(\"\\n=== Evaluation for Zero-Shot Summarization ===\")\n",
    "zero_shot_rouge = evaluate_with_rouge([generated_summary], [test_data[0][\"highlights\"]])\n",
    "zero_shot_bertscore = evaluate_with_bertscore([generated_summary], [test_data[0][\"highlights\"]])\n",
    "\n",
    "print(\"Zero-Shot ROUGE Scores:\", zero_shot_rouge)\n",
    "print(\"Zero-Shot BERTScore:\", zero_shot_bertscore)\n",
    "\n",
    "# Evaluate Few-Shot Summarization\n",
    "print(\"\\n=== Evaluation for Few-Shot Summarization ===\")\n",
    "few_shot_rouge = evaluate_with_rouge([generated_summary_fewshot], [test_data[0][\"highlights\"]])\n",
    "few_shot_bertscore = evaluate_with_bertscore([generated_summary_fewshot], [test_data[0][\"highlights\"]])\n",
    "\n",
    "print(\"Few-Shot ROUGE Scores:\", few_shot_rouge)\n",
    "print(\"Few-Shot BERTScore:\", few_shot_bertscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "\n",
    "*بخش دوم:*\n",
    "---\n",
    "1. نتایج پس از تنظیم دقیق (Fine-Tuning):\n",
    "\n",
    "    * ROUGE Scores:\n",
    "        \n",
    "        rouge1: 0.2131\n",
    "        rouge2: 0.1101\n",
    "        rougeL: 0.1536\n",
    "    \n",
    "    * BERTScore:\n",
    "        \n",
    "        Precision: 0.8080\n",
    "        Recall: 0.8784\n",
    "        F1: 0.8416\n",
    "\n",
    "2. مقایسه با Zero-Shot و Few-Shot:\n",
    "\n",
    "    * بهبود در ROUGE:\n",
    "        \n",
    "        rouge1 از 0.1036 (Zero-Shot) به 0.2131 افزایش یافته است.\n",
    "        \n",
    "        rouge2 نیز از 0.0693 (Zero-Shot) به 0.1101 بهبود یافته است.\n",
    "        \n",
    "        این نشان‌دهنده افزایش شباهت ن-گرم‌ها و ساختارهای زبانی میان خلاصه تولیدی و متن مرجع است.\n",
    "    \n",
    "    * BERTScore:\n",
    "        \n",
    "        F1 در مقایسه با Zero-Shot (0.8406) تقریباً ثابت مانده است (0.8416)، اما همچنان نسبت به Few-Shot (0.7731) بهتر است.\n",
    "        \n",
    "        Precision و Recall نیز بهبودهای اندکی نشان می‌دهند.\n",
    "\n",
    "در نهایت، تنظیم دقیق مدل به طور واضح عملکرد را در خلاصه‌سازی متون بهبود داده است.\n",
    "\n",
    "---\n",
    "\n",
    "مفاهیم:\n",
    "\n",
    "1. سناریوی استفاده از مدل‌های عام (بدون آموزش) و ضرورت تنظیم دقیق:\n",
    "\n",
    "    * استفاده از مدل‌های بزرگ عمومی:\n",
    "        \n",
    "        مناسب برای وظایف عمومی که نیاز به تطبیق خاصی ندارند (مانند ترجمه ساده یا خلاصه‌سازی عمومی).\n",
    "        \n",
    "        در محیط‌هایی با منابع محدود یا زمانی که داده کافی برای تنظیم دقیق در دسترس نیست.\n",
    "    * ضرورت تنظیم دقیق:\n",
    "        \n",
    "        در وظایف خاص (مانند خلاصه‌سازی تخصصی یا شناسایی موجودیت‌ها) تنظیم دقیق ضروری است.\n",
    "        \n",
    "        باعث می‌شود مدل به داده‌ها و سبک خاص آن وظیفه تطبیق پیدا کند و دقت به‌طور قابل توجهی بهبود یابد.\n",
    "\n",
    "---\n",
    "2. تاثیر استفاده از FP16 بر کارایی مدل و نحوه عملکرد:\n",
    "\n",
    "    * تاثیر:\n",
    "        \n",
    "        کاهش مصرف حافظه تا نصف.\n",
    "        \n",
    "        افزایش سرعت محاسبات به دلیل استفاده از عملیات‌های 16 بیتی.\n",
    "    * چالش‌ها:\n",
    "        \n",
    "        ممکن است دقت در وظایف حساس (مانند طبقه‌بندی دقیق) کاهش یابد.\n",
    "        \n",
    "        نیاز به پشتیبانی سخت‌افزاری (مانند GPUهای جدیدتر).\n",
    "    * نحوه عملکرد:\n",
    "        \n",
    "        در FP16 اعداد با دقت نصف ذخیره می‌شوند و عملیات‌ها سریع‌تر انجام می‌گیرند.\n",
    "        \n",
    "        مناسب برای تنظیم مدل‌های بزرگ در محیط‌های محدود.\n",
    "\n",
    "---\n",
    "3. مدل‌های زبانی سری SMOLLM2 و معماری آن‌ها:\n",
    "\n",
    "    * معرفی SMOLLM2:\n",
    "        \n",
    "        طراحی شده برای کارایی بالا در دستگاه‌های کم‌منبع.\n",
    "        \n",
    "        ترکیبی از مدل‌های سبک‌تر با قابلیت‌های رقابتی.\n",
    "    * معماری:\n",
    "        \n",
    "        استفاده از فشرده‌سازی پارامترها.\n",
    "        \n",
    "        پشتیبانی از FP16 و تکنیک‌های کاهش محاسبات.\n",
    "\n",
    "        عملکرد نزدیک به مدل‌های بزرگ‌تر در بسیاری از وظایف.\n",
    "\n",
    "---\n",
    "4. تکنیک‌های آموزش مدل‌های کوچک زبانی:\n",
    "\n",
    "    * تکنیک‌های موثر:\n",
    "        \n",
    "        کوانتایزر: کاهش دقت محاسبات (مانند 4 بیتی) برای بهبود سرعت.\n",
    "        \n",
    "        Transfer Learning: استفاده از مدل‌های بزرگ برای آموزش مدل‌های کوچک‌تر.\n",
    "        \n",
    "        فشرده‌سازی: کاهش تعداد پارامترها بدون افت دقت زیاد.\n",
    "    * در SMOLLM2:\n",
    "        \n",
    "        تمرکز بر تعادل بین کاهش اندازه و حفظ دقت.\n",
    "        \n",
    "        استفاده از تنظیم دقیق با داده‌های خاص وظیفه.\n",
    "\n",
    "---\n",
    "5. مقایسه نسخه ساده و نسخه Instruct-Tune:\n",
    "\n",
    "    * نسخه ساده:\n",
    "        \n",
    "        برای وظایف عمومی مناسب است.\n",
    "        \n",
    "        سریع‌تر و نیازمند منابع کمتر.\n",
    "    * نسخه Instruct-Tune:\n",
    "        \n",
    "        برای وظایف تخصصی و خاص، تنظیم شده است.\n",
    "        \n",
    "        دقت و کیفیت بالاتری دارد اما نیاز به آموزش اضافی دارد.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3_finetuned\",       # Directory to save the model\n",
    "    evaluation_strategy=\"steps\",          # Evaluate at every X steps\n",
    "    save_strategy=\"steps\",                # Save checkpoints\n",
    "    logging_dir=\"./logs\",                 # Directory for logs\n",
    "    learning_rate=2e-5,                   # Learning rate\n",
    "    per_device_train_batch_size=4,        # Batch size per GPU\n",
    "    per_device_eval_batch_size=4,         # Batch size for evaluation\n",
    "    num_train_epochs=3,                   # Number of epochs\n",
    "    logging_steps=500,                    # Log every 500 steps\n",
    "    save_steps=1000,                      # Save model every 1000 steps\n",
    "    eval_steps=500,                       # Evaluate model every 500 steps\n",
    "    warmup_steps=500,                     # Warmup steps\n",
    "    fp16=True,                            # Use 16-bit precision\n",
    "    report_to=\"tensorboard\",              # Log to TensorBoard\n",
    "    load_best_model_at_end=True,          # Load best model after training\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f38294a0ba4f14852407d3a6429fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a83bf556dd47f48575b863f3fbd6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 2:23:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.648200</td>\n",
       "      <td>2.524643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.513100</td>\n",
       "      <td>2.498239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.485400</td>\n",
       "      <td>2.486017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.467900</td>\n",
       "      <td>2.479134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.485600</td>\n",
       "      <td>2.473739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.458300</td>\n",
       "      <td>2.472174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.459100</td>\n",
       "      <td>2.470109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at ./llama3_finetuned/checkpoint-3500/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=2.49962412109375, metrics={'train_runtime': 8582.5291, 'train_samples_per_second': 1.748, 'train_steps_per_second': 0.437, 'total_flos': 8.792210827876762e+16, 'train_loss': 2.49962412109375, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], #specific to Llama models.\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./llama3_finetuned\")\n",
    "tokenizer.save_pretrained(\"./llama3_finetuned\")\n",
    "\n",
    "print(\"Fine-tuned model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: kaggle/working/llama3_finetuned/ (stored 0%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3000/ (stored 0%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3000/tokenizer.json (deflated 85%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3000/scheduler.pt (deflated 56%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3000/optimizer.pt (deflated 9%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3000/README.md (deflated 66%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3000/tokenizer_config.json (deflated 94%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3000/trainer_state.json (deflated 72%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3000/special_tokens_map.json (deflated 63%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3000/adapter_model.safetensors (deflated 8%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3000/adapter_config.json (deflated 54%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3000/training_args.bin (deflated 52%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3000/rng_state.pth (deflated 25%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3750/ (stored 0%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3750/tokenizer.json (deflated 85%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3750/scheduler.pt (deflated 56%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3750/optimizer.pt (deflated 9%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3750/README.md (deflated 66%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3750/tokenizer_config.json (deflated 94%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3750/trainer_state.json (deflated 74%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3750/special_tokens_map.json (deflated 63%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3750/adapter_model.safetensors (deflated 8%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3750/adapter_config.json (deflated 54%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3750/training_args.bin (deflated 52%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-3750/rng_state.pth (deflated 25%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-2000/ (stored 0%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-2000/tokenizer.json (deflated 85%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-2000/scheduler.pt (deflated 55%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-2000/optimizer.pt (deflated 9%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-2000/README.md (deflated 66%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-2000/tokenizer_config.json (deflated 94%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-2000/trainer_state.json (deflated 69%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-2000/special_tokens_map.json (deflated 63%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-2000/adapter_model.safetensors (deflated 8%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-2000/adapter_config.json (deflated 54%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-2000/training_args.bin (deflated 52%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-2000/rng_state.pth (deflated 25%)\n",
      "updating: kaggle/working/llama3_finetuned/tokenizer.json (deflated 85%)\n",
      "updating: kaggle/working/llama3_finetuned/README.md (deflated 66%)\n",
      "updating: kaggle/working/llama3_finetuned/tokenizer_config.json (deflated 94%)\n",
      "updating: kaggle/working/llama3_finetuned/special_tokens_map.json (deflated 63%)\n",
      "updating: kaggle/working/llama3_finetuned/adapter_model.safetensors (deflated 8%)\n",
      "updating: kaggle/working/llama3_finetuned/adapter_config.json (deflated 54%)\n",
      "updating: kaggle/working/llama3_finetuned/training_args.bin (deflated 52%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-1000/ (stored 0%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-1000/tokenizer.json (deflated 85%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-1000/scheduler.pt (deflated 55%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-1000/optimizer.pt (deflated 8%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-1000/README.md (deflated 66%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-1000/tokenizer_config.json (deflated 94%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-1000/trainer_state.json (deflated 62%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-1000/special_tokens_map.json (deflated 63%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-1000/adapter_model.safetensors (deflated 8%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-1000/adapter_config.json (deflated 54%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-1000/training_args.bin (deflated 52%)\n",
      "updating: kaggle/working/llama3_finetuned/checkpoint-1000/rng_state.pth (deflated 25%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r \"./working_directory_1.zip\" \"/kaggle/working/llama3_finetuned\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model and tokenizer loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_dir = \"/kaggle/input/llama3.1_finetuned_cnndailymail/pytorch/default/1/kaggle/working/llama3_finetuned/checkpoint-3750\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "\n",
    "print(\"Fine-tuned model and tokenizer loaded!\")\n",
    "\n",
    "\n",
    "# # Save the fine-tuned model\n",
    "# trainer.save_model(\"./llama3_finetuned\")\n",
    "# tokenizer.save_pretrained(\"./llama3_finetuned\")\n",
    "\n",
    "# print(\"Fine-tuned model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "def generate_summary_finetuned(model, tokenizer, input_text, max_new_tokens=150):\n",
    "    \"\"\"\n",
    "    Generates a summary for the input text using the fine-tuned model.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): The fine-tuned model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer for the model.\n",
    "        input_text (str): The text to summarize.\n",
    "        max_new_tokens (int): Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated summary.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=294\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate the summary\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "test_predictions = [\n",
    "    generate_summary_finetuned(model, tokenizer, article[\"article\"])\n",
    "    for article in test_data\n",
    "]\n",
    "\n",
    "test_references = [article[\"highlights\"] for article in test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Llama3.2 Summarization ===\n",
      "Reference Summary:\n",
      "Government to begin formal talks on funding Swansea Bay energy project .\n",
      "Scheme would be over area of four sq miles cordoned off by a breakwater .\n",
      "Power generated as tides pass through turbines when going in and out .\n",
      "\n",
      "Generated Summary:\n",
      "A £1billion tidal lagoon off the coast of south Wales will form the centrepiece of ambitious renewable energy plans to be unveiled today. George Osborne is set to use his Budget today to announce that the Government is beginning formal talks on funding the project to produce energy from turbines in Swansea Bay, south Wales. The scheme would be over an area of more than four square miles cordoned off by a breakwater, with power generated as the tides pass through turbines when going in and out. Scroll down for video. Construction: The project includes creating a six-mile sea reef, the reintroduction of the native oyster to Swansea Bay and an offshore visitor centre as well as national triathlon and water sports facilities. Below the surface: This graphic shows how the structure would generate energy at high tide, left, and low tide, right, meaning that water will flow through four times daily. Vast: The scheme would be over an area of more than four square miles of Swansea Bay in South Wales, creating a large tidal lagoon cordoned off by a huge breakwater. Artist's impression: Chancellor George Osborne is set to use his Budget today to announce a £1billion plan for the world's first tidal lagoon to generate electricity from turbines in Swansea Bay, south Wales. The Tidal Lagoon Power company will get the opportunity to discuss subsidies - which are expected to have to be about £150 per megawatt hour - with the Welsh Government, as well as with the UK's Department for Business, Energy and Industrial Strategy (BEIS) and the UK's Department for Environment, Food and Rural Affairs (Defra). The project is expected to create over 1,000 jobs in the region and will generate enough electricity to power more than 100,000 homes. The tidal lagoon would be the first of its kind in the world and would be the largest tidal lagoon in the UK. The project is expected to create over 1,000 jobs in the region and will generate enough electricity to power more than 100,000 homes. The tidal lagoon would be the first of its kind in the world and would be the largest tidal lagoon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Llama3.2 Summarization ===\")\n",
    "print(f\"Reference Summary:\\n{test_references[0]}\\n\")\n",
    "print(f\"Generated Summary:\\n{test_predictions[0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.21308312878557265, 'rouge2': 0.11005611674701608, 'rougeL': 0.15359150253258858}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308a87d8cd3b4f2e849a6ceab1f87674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308c4736c88d490481b21ff445e40322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e8784f912a4050a770f6fcb800fcb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6965332ea604e80a20aa8b14ff16630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ce2e2df4f64af2a1b912e4032b3bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb2cd3213524920b751f0c808b83334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore: {'precision': 0.8079621195793152, 'recall': 0.8783740997314453, 'f1': 0.8415759205818176}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "\n",
    "# Function to evaluate with ROUGE\n",
    "def evaluate_with_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    results = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        results[\"rouge1\"].append(scores[\"rouge1\"].fmeasure)\n",
    "        results[\"rouge2\"].append(scores[\"rouge2\"].fmeasure)\n",
    "        results[\"rougeL\"].append(scores[\"rougeL\"].fmeasure)\n",
    "    \n",
    "    # Calculate average ROUGE scores\n",
    "    avg_results = {metric: sum(scores) / len(scores) for metric, scores in results.items()}\n",
    "    return avg_results\n",
    "\n",
    "# Function to evaluate with BERTScore\n",
    "def evaluate_with_bertscore(predictions, references, lang=\"en\"):\n",
    "    P, R, F1 = score(predictions, references, lang=lang)\n",
    "    return {\"precision\": P.mean().item(), \"recall\": R.mean().item(), \"f1\": F1.mean().item()}\n",
    "\n",
    "# Evaluate ROUGE\n",
    "rouge_results = evaluate_with_rouge(test_predictions, test_references)\n",
    "print(\"ROUGE Scores:\", rouge_results)\n",
    "\n",
    "# Evaluate BERTScore\n",
    "bert_results = evaluate_with_bertscore(test_predictions, test_references, lang=\"en\")\n",
    "print(\"BERTScore:\", bert_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "\n",
    "*بخش سوم:*\n",
    "---\n",
    "1. عملکرد مدل هرس‌شده و مقایسه با مدل تنظیم‌شده:\n",
    "\n",
    "    * نتایج مدل هرس‌شده (Pruned Model):\n",
    "        \n",
    "        ROUGE:\n",
    "            \n",
    "            rouge1: 0.1675\n",
    "            rouge2: 0.0867\n",
    "            rougeL: 0.1225\n",
    "        \n",
    "        BERTScore:\n",
    "            \n",
    "            Precision: 0.7891\n",
    "            Recall: 0.8763\n",
    "            F1: 0.8302\n",
    "\n",
    "    * نتایج مدل تنظیم دقیق‌شده پس از هرس (Fine-Tuned Pruned Model):\n",
    "        \n",
    "        ROUGE:\n",
    "            \n",
    "            rouge1: 0.2129\n",
    "            rouge2: 0.1102\n",
    "            rougeL: 0.1529\n",
    "        \n",
    "        BERTScore:\n",
    "            \n",
    "            Precision: 0.8065\n",
    "            Recall: 0.8780\n",
    "            F1: 0.8406\n",
    "\n",
    "2. تحلیل تاثیر هرس بر عملکرد مدل:\n",
    "\n",
    "    * تاثیر منفی هرس:\n",
    "        \n",
    "        کاهش دقت اولیه در مدل هرس‌شده، به ویژه در مقادیر ROUGE (rouge1 و rouge2) مشاهده می‌شود.\n",
    "        \n",
    "        BERTScore نیز افت کمی در F1 و Precision نشان می‌دهد.\n",
    "        \n",
    "        این افت ناشی از حذف 30 درصد از وزن‌های مدل است که برخی از آنها اطلاعات مهم را شامل می‌شده‌اند.\n",
    "\n",
    "3. تاثیر تنظیم دقیق پس از هرس:\n",
    "\n",
    "    * تنظیم دقیق مدل هرس‌شده باعث بهبود قابل توجه در تمامی متریک‌ها شده است:\n",
    "        \n",
    "        ROUGE بهبود یافته و به مقادیر نزدیک به مدل اصلی تنظیم‌شده بازگشته است.\n",
    "        \n",
    "        BERTScore نیز بهبود یافته و عملکرد مدل را به سطح اولیه نزدیک کرده است.\n",
    "        \n",
    "        تنظیم دقیق باعث می‌شود مدل مجدداً اطلاعات ضروری را با توجه به داده‌های خاص وظیفه یاد بگیرد و اثرات منفی هرس را کاهش دهد.\n",
    "\n",
    "\n",
    "    مقادیر ROUGE و BERTScore مدل هرس‌شده پس از تنظیم دقیق، نزدیک به مدل تنظیم‌شده اولیه بدون هرس است.\n",
    "    این نشان می‌دهد که تنظیم دقیق می‌تواند اثرات منفی هرس (مانند حذف اطلاعات مهم) را جبران کند.\n",
    "\n",
    "---\n",
    "\n",
    "مفاهیم:\n",
    "\n",
    "1. تکنیک‌های مختلف هرس مدل‌های زبانی:\n",
    "\n",
    "    \n",
    "    * هرس بدون ساختار (Unstructured Pruning): حذف وزن‌های غیرضروری بدون توجه به ساختار مدل.\n",
    "        \n",
    "        مزایا: کاهش مصرف حافظه و افزایش sparsity.\n",
    "        \n",
    "        معایب: پیچیدگی سخت‌افزاری و نیاز به تنظیم دقیق بیشتر.\n",
    "    \n",
    "    * هرس ساختاری (Structured Pruning): حذف کامل بخش‌هایی از مدل (مانند لایه‌ها یا نودها).\n",
    "        \n",
    "        مزایا: سادگی در اجرا و افزایش سرعت.\n",
    "        \n",
    "        معایب: کاهش دقت بیشتر.\n",
    "    \n",
    "    * هرس ترکیبی (Global Sparsity): ترکیب هرس محلی و سراسری.\n",
    "        \n",
    "        مزایا: انعطاف‌پذیری بیشتر.\n",
    "        \n",
    "        معایب: پیچیدگی بالای تنظیم.\n",
    "\n",
    "---\n",
    "2. مقایسه با روش‌های دیگر فشرده‌سازی:\n",
    "\n",
    "    \n",
    "    * Quantization: کاهش دقت اعداد (مثل FP16 یا 4 بیتی).\n",
    "        \n",
    "        مزایا: کاهش حافظه و سرعت بالا.\n",
    "        \n",
    "        معایب: افت بیشتر در وظایف حساس.\n",
    "    \n",
    "    * Distillation: انتقال دانش از مدل بزرگ‌تر به مدل کوچک‌تر.\n",
    "        \n",
    "        مزایا: کیفیت مناسب در مدل کوچک‌تر.\n",
    "    \n",
    "    * Low-rank Factorization: کاهش پارامترها با تجزیه ماتریسی.\n",
    "        \n",
    "        مزایا: حفظ تعادل بین فشرده‌سازی و دقت.\n",
    "\n",
    "---\n",
    "3. اثر هرس بر عملکرد:\n",
    "\n",
    "    \n",
    "    * کاهش حافظه و هزینه محاسبات: به دلیل کاهش وزن‌ها و پارامترها.\n",
    "    \n",
    "    * افت دقت اولیه: ناشی از حذف اطلاعات مهم.\n",
    "\n",
    "---\n",
    "4. تنظیم دقیق پس از هرس:\n",
    "\n",
    "    \n",
    "    * با بازآموزی مدل، افت عملکرد جبران شده و دقت مدل بهبود می‌یابد.\n",
    "        \n",
    "    * این مرحله ضروری است تا مدل بر روی داده‌های وظیفه خاص تطبیق یابد.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Sparsity after Pruning: 30.00%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "def apply_pruning(model, amount=0.3):\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, \"weight\") and isinstance(module, (torch.nn.Linear, torch.nn.Conv2d)):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "            prune.remove(module, \"weight\")\n",
    "    return model\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "model = apply_pruning(model)\n",
    "\n",
    "# Calculate sparsity\n",
    "def calculate_sparsity(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    zero_params = sum(torch.sum(p == 0).item() for p in model.parameters())\n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity\n",
    "\n",
    "\n",
    "sparsity = calculate_sparsity(model)\n",
    "print(f\"Model Sparsity after Pruning: {sparsity * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer                                    Sparsity (%)    Zero Weights    Total Weights  \n",
      "--------------------------------------------------------------------------------\n",
      "model.layers.0.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.0.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.0.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.0.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.0.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.0.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.0.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.0.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.0.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.0.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.0.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.0.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.0.mlp.gate_proj             30.00           5033165         16777216       \n",
      "model.layers.0.mlp.up_proj               30.00           5033165         16777216       \n",
      "model.layers.0.mlp.down_proj             30.00           5033165         16777216       \n",
      "model.layers.1.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.1.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.1.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.1.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.1.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.1.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.1.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.1.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.1.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.1.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.1.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.1.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.1.mlp.gate_proj             30.00           5033165         16777216       \n",
      "model.layers.1.mlp.up_proj               30.00           5033165         16777216       \n",
      "model.layers.1.mlp.down_proj             30.00           5033165         16777216       \n",
      "model.layers.2.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.2.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.2.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.2.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.2.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.2.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.2.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.2.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.2.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.2.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.2.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.2.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.2.mlp.gate_proj             30.00           5033165         16777216       \n",
      "model.layers.2.mlp.up_proj               30.00           5033165         16777216       \n",
      "model.layers.2.mlp.down_proj             30.00           5033165         16777216       \n",
      "model.layers.3.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.3.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.3.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.3.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.3.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.3.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.3.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.3.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.3.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.3.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.3.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.3.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.3.mlp.gate_proj             30.00           5033165         16777216       \n",
      "model.layers.3.mlp.up_proj               30.00           5033165         16777216       \n",
      "model.layers.3.mlp.down_proj             30.00           5033165         16777216       \n",
      "model.layers.4.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.4.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.4.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.4.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.4.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.4.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.4.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.4.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.4.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.4.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.4.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.4.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.4.mlp.gate_proj             30.00           5033165         16777216       \n",
      "model.layers.4.mlp.up_proj               30.00           5033165         16777216       \n",
      "model.layers.4.mlp.down_proj             30.00           5033165         16777216       \n",
      "model.layers.5.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.5.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.5.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.5.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.5.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.5.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.5.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.5.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.5.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.5.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.5.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.5.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.5.mlp.gate_proj             30.00           5033165         16777216       \n",
      "model.layers.5.mlp.up_proj               30.00           5033165         16777216       \n",
      "model.layers.5.mlp.down_proj             30.00           5033165         16777216       \n",
      "model.layers.6.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.6.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.6.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.6.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.6.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.6.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.6.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.6.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.6.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.6.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.6.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.6.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.6.mlp.gate_proj             30.00           5033165         16777216       \n",
      "model.layers.6.mlp.up_proj               30.00           5033165         16777216       \n",
      "model.layers.6.mlp.down_proj             30.00           5033165         16777216       \n",
      "model.layers.7.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.7.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.7.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.7.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.7.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.7.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.7.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.7.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.7.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.7.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.7.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.7.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.7.mlp.gate_proj             30.00           5033165         16777216       \n",
      "model.layers.7.mlp.up_proj               30.00           5033165         16777216       \n",
      "model.layers.7.mlp.down_proj             30.00           5033165         16777216       \n",
      "model.layers.8.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.8.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.8.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.8.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.8.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.8.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.8.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.8.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.8.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.8.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.8.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.8.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.8.mlp.gate_proj             30.00           5033165         16777216       \n",
      "model.layers.8.mlp.up_proj               30.00           5033165         16777216       \n",
      "model.layers.8.mlp.down_proj             30.00           5033165         16777216       \n",
      "model.layers.9.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.9.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.9.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.9.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.9.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.9.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.9.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.9.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.9.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.9.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.9.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.9.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.9.mlp.gate_proj             30.00           5033165         16777216       \n",
      "model.layers.9.mlp.up_proj               30.00           5033165         16777216       \n",
      "model.layers.9.mlp.down_proj             30.00           5033165         16777216       \n",
      "model.layers.10.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.10.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.10.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.10.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.10.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.10.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.10.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.10.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.10.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.10.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.10.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.10.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.10.mlp.gate_proj            30.00           5033165         16777216       \n",
      "model.layers.10.mlp.up_proj              30.00           5033165         16777216       \n",
      "model.layers.10.mlp.down_proj            30.00           5033165         16777216       \n",
      "model.layers.11.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.11.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.11.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.11.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.11.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.11.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.11.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.11.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.11.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.11.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.11.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.11.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.11.mlp.gate_proj            30.00           5033165         16777216       \n",
      "model.layers.11.mlp.up_proj              30.00           5033165         16777216       \n",
      "model.layers.11.mlp.down_proj            30.00           5033165         16777216       \n",
      "model.layers.12.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.12.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.12.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.12.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.12.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.12.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.12.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.12.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.12.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.12.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.12.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.12.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.12.mlp.gate_proj            30.00           5033165         16777216       \n",
      "model.layers.12.mlp.up_proj              30.00           5033165         16777216       \n",
      "model.layers.12.mlp.down_proj            30.00           5033165         16777216       \n",
      "model.layers.13.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.13.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.13.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.13.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.13.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.13.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.13.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.13.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.13.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.13.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.13.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.13.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.13.mlp.gate_proj            30.00           5033165         16777216       \n",
      "model.layers.13.mlp.up_proj              30.00           5033165         16777216       \n",
      "model.layers.13.mlp.down_proj            30.00           5033165         16777216       \n",
      "model.layers.14.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.14.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.14.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.14.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.14.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.14.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.14.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.14.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.14.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.14.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.14.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.14.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.14.mlp.gate_proj            30.00           5033165         16777216       \n",
      "model.layers.14.mlp.up_proj              30.00           5033165         16777216       \n",
      "model.layers.14.mlp.down_proj            30.00           5033165         16777216       \n",
      "model.layers.15.self_attn.q_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.15.self_attn.q_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.15.self_attn.q_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.15.self_attn.k_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.15.self_attn.k_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.15.self_attn.k_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.15.self_attn.v_proj.base_layer 30.00           314573          1048576        \n",
      "model.layers.15.self_attn.v_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.15.self_attn.v_proj.lora_B.default 30.00           2458            8192           \n",
      "model.layers.15.self_attn.o_proj.base_layer 30.00           1258291         4194304        \n",
      "model.layers.15.self_attn.o_proj.lora_A.default 30.00           9830            32768          \n",
      "model.layers.15.self_attn.o_proj.lora_B.default 30.00           9830            32768          \n",
      "model.layers.15.mlp.gate_proj            30.00           5033165         16777216       \n",
      "model.layers.15.mlp.up_proj              30.00           5033165         16777216       \n",
      "model.layers.15.mlp.down_proj            30.00           5033165         16777216       \n",
      "lm_head                                  30.00           78800486        262668288      \n",
      "--------------------------------------------------------------------------------\n",
      "Overall Sparsity                         30.00           371746390       1239154688     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.999998676517134"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def check_sparsity(model):\n",
    "    \"\"\"\n",
    "    Check the sparsity of the pruned model.\n",
    "    Prints the sparsity for each layer and overall sparsity.\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "\n",
    "    print(f\"{'Layer':<40} {'Sparsity (%)':<15} {'Zero Weights':<15} {'Total Weights':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Get the total number of weights and the number of zero weights\n",
    "            total_weights = module.weight.numel()\n",
    "            zero_weights = (module.weight == 0).sum().item()\n",
    "\n",
    "            # Compute sparsity for this layer\n",
    "            sparsity = 100.0 * zero_weights / total_weights\n",
    "            print(f\"{name:<40} {sparsity:<15.2f} {zero_weights:<15} {total_weights:<15}\")\n",
    "            \n",
    "            # Update overall counts\n",
    "            total_params += total_weights\n",
    "            zero_params += zero_weights\n",
    "    \n",
    "    # Compute overall sparsity\n",
    "    overall_sparsity = 100.0 * zero_params / total_params\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Overall Sparsity':<40} {overall_sparsity:<15.2f} {zero_params:<15} {total_params:<15}\")\n",
    "    return overall_sparsity\n",
    "\n",
    "check_sparsity(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Model ROUGE Scores: {'rouge1': 0.16751959284934764, 'rouge2': 0.0867393795313324, 'rougeL': 0.12249557089842027}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Model BERTScore: {'precision': 0.7890657782554626, 'recall': 0.8762536644935608, 'f1': 0.8301979899406433}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pruned_model = pruned_model.to(device)\n",
    "\n",
    "def generate_summary_finetuned(model, tokenizer, input_text, max_new_tokens=294):\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=294\n",
    "    ).to(device) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_summary\n",
    "\n",
    "pruned_predictions = [\n",
    "    generate_summary_finetuned(pruned_model, tokenizer, article[\"article\"])\n",
    "    for article in test_data\n",
    "]\n",
    "\n",
    "# Evaluate ROUGE for pruned model\n",
    "pruned_rouge_results = evaluate_with_rouge(pruned_predictions, test_references)\n",
    "print(\"Pruned Model ROUGE Scores:\", pruned_rouge_results)\n",
    "\n",
    "# Evaluate BERTScore for pruned model\n",
    "pruned_bert_results = evaluate_with_bertscore(pruned_predictions, test_references, lang=\"en\")\n",
    "print(\"Pruned Model BERTScore:\", pruned_bert_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10e4e2a126f4996bb23927512e09d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735cd811a10d4e0d9eac1028f47a2f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 4:33:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.867100</td>\n",
       "      <td>2.617118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.584400</td>\n",
       "      <td>2.572099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.575600</td>\n",
       "      <td>2.557248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.544300</td>\n",
       "      <td>2.546072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.536000</td>\n",
       "      <td>2.536603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.510100</td>\n",
       "      <td>2.530398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.509900</td>\n",
       "      <td>2.525460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.497700</td>\n",
       "      <td>2.522212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.530000</td>\n",
       "      <td>2.519343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.512900</td>\n",
       "      <td>2.515479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.499100</td>\n",
       "      <td>2.514967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.486700</td>\n",
       "      <td>2.513633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.489600</td>\n",
       "      <td>2.512645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.494400</td>\n",
       "      <td>2.510950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.491700</td>\n",
       "      <td>2.510952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=2.5419594401041667, metrics={'train_runtime': 16387.8792, 'train_samples_per_second': 0.915, 'train_steps_per_second': 0.458, 'total_flos': 8.21527015593984e+16, 'train_loss': 2.5419594401041667, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3_finetuned\",       # Directory to save the model\n",
    "    evaluation_strategy=\"steps\",          # Evaluate at every X steps\n",
    "    save_strategy=\"steps\",                # Save checkpoints\n",
    "    logging_dir=\"./logs\",                 # Directory for logs\n",
    "    learning_rate=2e-5,                   # Learning rate\n",
    "    per_device_train_batch_size=2,        # Batch size per GPU\n",
    "    per_device_eval_batch_size=2,         # Batch size for evaluation\n",
    "    num_train_epochs=3,                   # Number of epochs\n",
    "    logging_steps=500,                    # Log every 500 steps\n",
    "    save_steps=1000,                      # Save model every 1000 steps\n",
    "    eval_steps=500,                       # Evaluate model every 500 steps\n",
    "    warmup_steps=500,                     # Warmup steps\n",
    "    fp16=True,                            # Use 16-bit precision\n",
    "    report_to=\"tensorboard\",              # Log to TensorBoard\n",
    "    load_best_model_at_end=True,          # Load best model after training\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # Disable caching\n",
    "\n",
    "pruned_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Fine-tune the pruned model\n",
    "pruned_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_references' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\n",
      "\u001b[0;32m<ipython-input-43-a40bf565210f>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n",
      "\n",
      "\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m      7\u001b[0m \u001b[0;31m# Evaluate ROUGE for fine-tuned pruned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfine_tuned_pruned_rouge_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_with_rouge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfine_tuned_pruned_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_references\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fine-Tuned Pruned Model ROUGE Scores:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_tuned_pruned_rouge_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_references' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate summaries using the fine-tuned pruned model\n",
    "fine_tuned_pruned_predictions = [\n",
    "    generate_summary_finetuned(model, tokenizer, article[\"article\"])\n",
    "    for article in test_data\n",
    "]\n",
    "\n",
    "# Evaluate ROUGE for fine-tuned pruned model\n",
    "fine_tuned_pruned_rouge_results = evaluate_with_rouge(fine_tuned_pruned_predictions, test_references)\n",
    "print(\"Fine-Tuned Pruned Model ROUGE Scores:\", fine_tuned_pruned_rouge_results)\n",
    "\n",
    "# Evaluate BERTScore for fine-tuned pruned model\n",
    "fine_tuned_pruned_bert_results = evaluate_with_bertscore(fine_tuned_pruned_predictions, test_references, lang=\"en\")\n",
    "print(\"Fine-Tuned Pruned Model BERTScore:\", fine_tuned_pruned_bert_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Pruned Model ROUGE Scores: {'rouge1': 0.21287360187418508, 'rouge2': 0.11021003969683907, 'rougeL': 0.152882660443788}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Pruned Model BERTScore: {'precision': 0.806499719619751, 'recall': 0.8780384063720703, 'f1': 0.84063321352005}\n"
     ]
    }
   ],
   "source": [
    "# Extract references (gold summaries)\n",
    "test_references = [article[\"highlights\"] for article in test_data]\n",
    "\n",
    "# Evaluate ROUGE for fine-tuned pruned model\n",
    "fine_tuned_pruned_rouge_results = evaluate_with_rouge(fine_tuned_pruned_predictions, test_references)\n",
    "print(\"Fine-Tuned Pruned Model ROUGE Scores:\", fine_tuned_pruned_rouge_results)\n",
    "\n",
    "# Evaluate BERTScore for fine-tuned pruned model\n",
    "fine_tuned_pruned_bert_results = evaluate_with_bertscore(fine_tuned_pruned_predictions, test_references, lang=\"en\")\n",
    "print(\"Fine-Tuned Pruned Model BERTScore:\", fine_tuned_pruned_bert_results)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
